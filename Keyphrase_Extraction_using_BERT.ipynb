{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Keyphrase Extraction using BERT.ipynb",
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyOLgT1vhC33ogWFi2WLmgdh",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AnshMittal1811/BERT/blob/master/Keyphrase_Extraction_using_BERT.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0d2Z4Gf8pPN6",
        "colab_type": "text"
      },
      "source": [
        "# Assignment for Rapidken.AI"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GTHGFf6MLaPq",
        "colab_type": "code",
        "outputId": "749ac8da-8065-4bdd-b2ea-7d52b09550f3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "!ls"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "sample_data\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KgK6qHLxbzuI",
        "colab_type": "text"
      },
      "source": [
        "# Task 2\n",
        "Using the model trained and built using Pranav's code, please extract key phrases the text file."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1a8gwGLcV_Sf",
        "colab_type": "text"
      },
      "source": [
        "### Trying SciBERT according to the Instructions in BERT-keyphrase-extraction Repository"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yr2cfj1iLGB2",
        "colab_type": "code",
        "outputId": "938b3311-ef17-4d3e-a81a-431aa6c9d93c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "!git clone https://github.com/pranav-ust/BERT-keyphrase-extraction"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'BERT-keyphrase-extraction'...\n",
            "remote: Enumerating objects: 77, done.\u001b[K\n",
            "Unpacking objects:   1% (1/77)   \rUnpacking objects:   2% (2/77)   \rUnpacking objects:   3% (3/77)   \rUnpacking objects:   5% (4/77)   \rUnpacking objects:   6% (5/77)   \rUnpacking objects:   7% (6/77)   \rUnpacking objects:   9% (7/77)   \rUnpacking objects:  10% (8/77)   \rUnpacking objects:  11% (9/77)   \rUnpacking objects:  12% (10/77)   \rUnpacking objects:  14% (11/77)   \rUnpacking objects:  15% (12/77)   \rUnpacking objects:  16% (13/77)   \rUnpacking objects:  18% (14/77)   \rUnpacking objects:  19% (15/77)   \rUnpacking objects:  20% (16/77)   \rUnpacking objects:  22% (17/77)   \rUnpacking objects:  23% (18/77)   \rUnpacking objects:  24% (19/77)   \rUnpacking objects:  25% (20/77)   \rUnpacking objects:  27% (21/77)   \rUnpacking objects:  28% (22/77)   \rUnpacking objects:  29% (23/77)   \rUnpacking objects:  31% (24/77)   \rUnpacking objects:  32% (25/77)   \rUnpacking objects:  33% (26/77)   \rUnpacking objects:  35% (27/77)   \rUnpacking objects:  36% (28/77)   \rUnpacking objects:  37% (29/77)   \rUnpacking objects:  38% (30/77)   \rUnpacking objects:  40% (31/77)   \rUnpacking objects:  41% (32/77)   \rUnpacking objects:  42% (33/77)   \rUnpacking objects:  44% (34/77)   \rUnpacking objects:  45% (35/77)   \rUnpacking objects:  46% (36/77)   \rUnpacking objects:  48% (37/77)   \rUnpacking objects:  49% (38/77)   \rUnpacking objects:  50% (39/77)   \rUnpacking objects:  51% (40/77)   \rUnpacking objects:  53% (41/77)   \rUnpacking objects:  54% (42/77)   \rUnpacking objects:  55% (43/77)   \rUnpacking objects:  57% (44/77)   \rUnpacking objects:  58% (45/77)   \rUnpacking objects:  59% (46/77)   \rUnpacking objects:  61% (47/77)   \rUnpacking objects:  62% (48/77)   \rUnpacking objects:  63% (49/77)   \rUnpacking objects:  64% (50/77)   \rUnpacking objects:  66% (51/77)   \rUnpacking objects:  67% (52/77)   \rUnpacking objects:  68% (53/77)   \rUnpacking objects:  70% (54/77)   \rUnpacking objects:  71% (55/77)   \rUnpacking objects:  72% (56/77)   \rUnpacking objects:  74% (57/77)   \rUnpacking objects:  75% (58/77)   \rUnpacking objects:  76% (59/77)   \rUnpacking objects:  77% (60/77)   \rUnpacking objects:  79% (61/77)   \rUnpacking objects:  80% (62/77)   \rUnpacking objects:  81% (63/77)   \rUnpacking objects:  83% (64/77)   \rUnpacking objects:  84% (65/77)   \rUnpacking objects:  85% (66/77)   \rUnpacking objects:  87% (67/77)   \rUnpacking objects:  88% (68/77)   \rUnpacking objects:  89% (69/77)   \rUnpacking objects:  90% (70/77)   \rUnpacking objects:  92% (71/77)   \rUnpacking objects:  93% (72/77)   \rUnpacking objects:  94% (73/77)   \rUnpacking objects:  96% (74/77)   \rremote: Total 77 (delta 0), reused 0 (delta 0), pack-reused 77\u001b[K\n",
            "Unpacking objects:  97% (75/77)   \rUnpacking objects:  98% (76/77)   \rUnpacking objects: 100% (77/77)   \rUnpacking objects: 100% (77/77), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8-LVVvkZLWZa",
        "colab_type": "code",
        "outputId": "20cc0ebf-633c-4107-82c4-05fa172b77fb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 411
        }
      },
      "source": [
        "!pip install pytorch-pretrained-BERT"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting pytorch-pretrained-BERT\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d7/e0/c08d5553b89973d9a240605b9c12404bcf8227590de62bae27acbcfe076b/pytorch_pretrained_bert-0.6.2-py3-none-any.whl (123kB)\n",
            "\r\u001b[K     |██▋                             | 10kB 21.0MB/s eta 0:00:01\r\u001b[K     |█████▎                          | 20kB 3.0MB/s eta 0:00:01\r\u001b[K     |████████                        | 30kB 4.0MB/s eta 0:00:01\r\u001b[K     |██████████▋                     | 40kB 4.2MB/s eta 0:00:01\r\u001b[K     |█████████████▎                  | 51kB 3.5MB/s eta 0:00:01\r\u001b[K     |███████████████▉                | 61kB 4.0MB/s eta 0:00:01\r\u001b[K     |██████████████████▌             | 71kB 4.1MB/s eta 0:00:01\r\u001b[K     |█████████████████████▏          | 81kB 4.6MB/s eta 0:00:01\r\u001b[K     |███████████████████████▉        | 92kB 4.7MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▌     | 102kB 4.7MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▏  | 112kB 4.7MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▊| 122kB 4.7MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 133kB 4.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from pytorch-pretrained-BERT) (1.18.4)\n",
            "Requirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (from pytorch-pretrained-BERT) (1.13.19)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from pytorch-pretrained-BERT) (4.41.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from pytorch-pretrained-BERT) (2.23.0)\n",
            "Requirement already satisfied: torch>=0.4.1 in /usr/local/lib/python3.6/dist-packages (from pytorch-pretrained-BERT) (1.5.0+cu101)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.6/dist-packages (from pytorch-pretrained-BERT) (2019.12.20)\n",
            "Requirement already satisfied: s3transfer<0.4.0,>=0.3.0 in /usr/local/lib/python3.6/dist-packages (from boto3->pytorch-pretrained-BERT) (0.3.3)\n",
            "Requirement already satisfied: botocore<1.17.0,>=1.16.19 in /usr/local/lib/python3.6/dist-packages (from boto3->pytorch-pretrained-BERT) (1.16.19)\n",
            "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->pytorch-pretrained-BERT) (0.10.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch-pretrained-BERT) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch-pretrained-BERT) (2.9)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch-pretrained-BERT) (2020.4.5.1)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch-pretrained-BERT) (1.24.3)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torch>=0.4.1->pytorch-pretrained-BERT) (0.16.0)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.6/dist-packages (from botocore<1.17.0,>=1.16.19->boto3->pytorch-pretrained-BERT) (2.8.1)\n",
            "Requirement already satisfied: docutils<0.16,>=0.10 in /usr/local/lib/python3.6/dist-packages (from botocore<1.17.0,>=1.16.19->boto3->pytorch-pretrained-BERT) (0.15.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.6/dist-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.17.0,>=1.16.19->boto3->pytorch-pretrained-BERT) (1.12.0)\n",
            "Installing collected packages: pytorch-pretrained-BERT\n",
            "Successfully installed pytorch-pretrained-BERT-0.6.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nr0W1G5dXFme",
        "colab_type": "code",
        "outputId": "fdde4cd5-a0c4-4480-b377-2608b20592d5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 224
        }
      },
      "source": [
        "!wget https://s3-us-west-2.amazonaws.com/ai2-s2-research/scibert/pytorch_models/scibert_scivocab_uncased.tar"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-06-06 20:59:10--  https://s3-us-west-2.amazonaws.com/ai2-s2-research/scibert/pytorch_models/scibert_scivocab_uncased.tar\n",
            "Resolving s3-us-west-2.amazonaws.com (s3-us-west-2.amazonaws.com)... 52.218.237.48\n",
            "Connecting to s3-us-west-2.amazonaws.com (s3-us-west-2.amazonaws.com)|52.218.237.48|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 410593280 (392M) [application/x-tar]\n",
            "Saving to: ‘scibert_scivocab_uncased.tar’\n",
            "\n",
            "scibert_scivocab_un 100%[===================>] 391.57M  51.7MB/s    in 7.6s    \n",
            "\n",
            "2020-06-06 20:59:18 (51.3 MB/s) - ‘scibert_scivocab_uncased.tar’ saved [410593280/410593280]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mMn_J6TncWcr",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "694535c2-d980-423a-cc3c-bcfcfe9766c3"
      },
      "source": [
        "!ls"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "BERT-keyphrase-extraction  sample_data\tscibert_scivocab_uncased.tar\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uevzGz6iXb_Q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!mkdir BERT-keyphrase-extraction/model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cquYFXzzZMd2",
        "colab_type": "code",
        "outputId": "1238e4cf-f354-433e-9b72-56c17fdce9d1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "!tar -C BERT-keyphrase-extraction/model -xvf scibert_scivocab_uncased.tar"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "scibert_scivocab_uncased/\n",
            "scibert_scivocab_uncased/weights.tar.gz\n",
            "scibert_scivocab_uncased/vocab.txt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5qbYnASwcnhI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "os.chdir('BERT-keyphrase-extraction/model/scibert_scivocab_uncased')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sMPlaiD2cwFT",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "8c04a028-53f8-4b36-fec2-fc807dbe17f0"
      },
      "source": [
        "!ls"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "vocab.txt  weights.tar.gz\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JRN96UJmeSkd",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "ac82c6aa-9e82-4b3e-ae9e-044df9ef3d6f"
      },
      "source": [
        "!tar -C ./ -xvf weights.tar.gz"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "bert_config.json\n",
            "pytorch_model.bin\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6vLFCyGigPhC",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "afd2ed98-3409-4bde-d9e7-7c0d7cd8dfd7"
      },
      "source": [
        "!ls"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "bert_config.json  pytorch_model.bin  vocab.txt\tweights.tar.gz\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BYsStO7-fimR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!mv pytorch_model.bin ../\n",
        "!mv vocab.txt ../\n",
        "!mv bert_config.json ../"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_Y--NAexhW55",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "os.chdir('../../')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wbFzBi7PhgBu",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "98fd419d-28f3-4cf0-8699-fdd08f2d465b"
      },
      "source": [
        "!ls"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "data\t\tevaluate.py  metrics.py  README.md  utils.py\n",
            "data_loader.py\texperiments  model\t train.py\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3gL9-TW_iy5J",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!rm -r model/scibert_scivocab_uncased"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zKOFy_ulcDWr",
        "colab_type": "text"
      },
      "source": [
        "Here, we change the `params.json` file in `experiments/base_model` to increase number of epochs to 50 along with `min_epoch_num : 25` and `patience_num : 10`. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-undX2ZEVw1c",
        "colab_type": "code",
        "outputId": "ac626f88-b3d4-4c67-dc40-0243e7aeda52",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "!python train.py --data_dir data/task1/ --bert_model_dir model/ --model_dir experiments/base_model"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "device: cuda, n_gpu: 1, 16-bits training: False\n",
            "Loading the datasets...\n",
            "loading vocabulary file model/vocab.txt\n",
            "loading archive file model/\n",
            "Model config {\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"vocab_size\": 31090\n",
            "}\n",
            "\n",
            "Weights of BertForTokenClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']\n",
            "Weights from pretrained model not used in BertForTokenClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
            "Starting training for 50 epoch(s)\n",
            "Epoch 1/50\n",
            "/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
            "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n",
            "100% 99/99 [00:13<00:00,  7.17it/s, loss=0.339]\n",
            "- Train metrics: loss: 00.17; f1: 46.43\n",
            "- Val metrics: loss: 00.29; f1: 34.42\n",
            "- Found new best F1\n",
            "Epoch 2/50\n",
            "100% 99/99 [00:13<00:00,  7.32it/s, loss=0.182]\n",
            "- Train metrics: loss: 00.08; f1: 58.81\n",
            "- Val metrics: loss: 00.30; f1: 39.06\n",
            "- Found new best F1\n",
            "Epoch 3/50\n",
            "100% 99/99 [00:13<00:00,  7.33it/s, loss=0.106]\n",
            "- Train metrics: loss: 00.04; f1: 67.12\n",
            "- Val metrics: loss: 00.29; f1: 42.79\n",
            "- Found new best F1\n",
            "Epoch 4/50\n",
            "100% 99/99 [00:13<00:00,  7.31it/s, loss=0.053]\n",
            "- Train metrics: loss: 00.04; f1: 67.35\n",
            "- Val metrics: loss: 00.49; f1: 38.40\n",
            "Epoch 5/50\n",
            "100% 99/99 [00:13<00:00,  7.42it/s, loss=0.036]\n",
            "- Train metrics: loss: 00.01; f1: 71.76\n",
            "- Val metrics: loss: 00.37; f1: 44.59\n",
            "- Found new best F1\n",
            "Epoch 6/50\n",
            "100% 99/99 [00:13<00:00,  7.45it/s, loss=0.023]\n",
            "- Train metrics: loss: 00.01; f1: 72.45\n",
            "- Val metrics: loss: 00.39; f1: 44.81\n",
            "- Found new best F1\n",
            "Epoch 7/50\n",
            "100% 99/99 [00:13<00:00,  7.44it/s, loss=0.015]\n",
            "- Train metrics: loss: 00.01; f1: 72.77\n",
            "- Val metrics: loss: 00.46; f1: 44.39\n",
            "Epoch 8/50\n",
            "100% 99/99 [00:13<00:00,  7.39it/s, loss=0.010]\n",
            "- Train metrics: loss: 00.00; f1: 73.57\n",
            "- Val metrics: loss: 00.53; f1: 42.70\n",
            "Epoch 9/50\n",
            "100% 99/99 [00:13<00:00,  7.42it/s, loss=0.006]\n",
            "- Train metrics: loss: 00.00; f1: 73.67\n",
            "- Val metrics: loss: 00.64; f1: 42.96\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y93-xNYyWe6l",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "d5838483-6abb-4f73-9e58-3d050f04e863"
      },
      "source": [
        "!python evaluate.py --data_dir data/task1/ --bert_model_dir model/ --model_dir experiments/base_model --restore_file best"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loading the dataset...\n",
            "loading vocabulary file model/vocab.txt\n",
            "- done.\n",
            "Starting evaluation...\n",
            "- Test metrics: loss: 00.37; f1: 46.06\n",
            "             precision    recall  f1-score   support\n",
            "\n",
            "          I      36.35     62.87     46.06       921\n",
            "\n",
            "avg / total      36.35     62.87     46.06       921\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NDd3wn8Xpf1u",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!rm -r model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C5ZCxGsbpsWv",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "54304563-10d0-49bc-80c6-acb689839173"
      },
      "source": [
        "!ls"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "data\t\tevaluate.py  metrics.py   README.md  utils.py\n",
            "data_loader.py\texperiments  __pycache__  train.py\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zS_Uf3_NykEh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "os.chdir('./experiments/base_model')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cRWsqOoqyr6n",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!rm ./best.pth.tar\n",
        "!rm ./evaluate.log\n",
        "!rm ./last.pth.tar\n",
        "!rm ./train.log"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kaXpFSLkbj3Q",
        "colab_type": "text"
      },
      "source": [
        "# Task 1 \n",
        "\n",
        "Use the code in\n",
        "https://github.com/pranav-ust/BERT-keyphrase-extraction to train it\n",
        "with default parameters and evaluate the model for just Subtask 1 with\n",
        "the following change: Instead of using SciBERT, please use BERT. As\n",
        "you can see, Pranav has given evaluation results with SciBERT as F1\n",
        "score: 0.6259; Precision: 0.5986; Recall: 0.6558; Support: 921. Please\n",
        "give the results using BERT instead."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hiwgG7mlpYHP",
        "colab_type": "text"
      },
      "source": [
        "### Trying BERT according using BERT-keyphrase-extraction Repository"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cSaJcPtammue",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "os.chdir('../../')\n",
        "!mkdir ./model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AFQPHh4rpz43",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 224
        },
        "outputId": "9527837c-91ba-4478-8c61-2f5348fe0e43"
      },
      "source": [
        "!wget https://storage.googleapis.com/bert_models/2018_10_18/uncased_L-24_H-1024_A-16.zip"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-06-06 18:39:11--  https://storage.googleapis.com/bert_models/2018_10_18/uncased_L-24_H-1024_A-16.zip\n",
            "Resolving storage.googleapis.com (storage.googleapis.com)... 64.233.167.128, 2a00:1450:400c:c0b::80\n",
            "Connecting to storage.googleapis.com (storage.googleapis.com)|64.233.167.128|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1247797031 (1.2G) [application/zip]\n",
            "Saving to: ‘uncased_L-24_H-1024_A-16.zip’\n",
            "\n",
            "uncased_L-24_H-1024 100%[===================>]   1.16G   179MB/s    in 7.1s    \n",
            "\n",
            "2020-06-06 18:39:18 (168 MB/s) - ‘uncased_L-24_H-1024_A-16.zip’ saved [1247797031/1247797031]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5Em1ZBmCq3k5",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        },
        "outputId": "83af74fb-4b66-4606-8139-73b936ba06ad"
      },
      "source": [
        "!unzip uncased_L-24_H-1024_A-16.zip -d ./model"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Archive:  uncased_L-24_H-1024_A-16.zip\n",
            "   creating: ./model/uncased_L-24_H-1024_A-16/\n",
            "  inflating: ./model/uncased_L-24_H-1024_A-16/bert_model.ckpt.meta  \n",
            "  inflating: ./model/uncased_L-24_H-1024_A-16/bert_model.ckpt.data-00000-of-00001  \n",
            "  inflating: ./model/uncased_L-24_H-1024_A-16/vocab.txt  \n",
            "  inflating: ./model/uncased_L-24_H-1024_A-16/bert_model.ckpt.index  \n",
            "  inflating: ./model/uncased_L-24_H-1024_A-16/bert_config.json  \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WJRR-bPBvQuV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!rm -r uncased_L-24_H-1024_A-16.zip"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0NmD78UKvqNS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "os.chdir('./model/uncased_L-24_H-1024_A-16')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GrzRnELovvul",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!mv bert_model.ckpt.meta ../\n",
        "!mv bert_model.ckpt.data-00000-of-00001 ../\n",
        "!mv bert_config.json ../\n",
        "!mv vocab.txt ../\n",
        "!mv bert_model.ckpt.index ../"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "egXN7_dtvvcv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!rm -r ../uncased_L-24_H-1024_A-16"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y0tKgDcavvZp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "os.chdir('/content/BERT-keyphrase-extraction/model')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rk46t8XFwj16",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "20665645-24ab-4e94-ba52-6cb48676fc23"
      },
      "source": [
        "!ls"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "bert_config.json\t\t     bert_model.ckpt.index  vocab.txt\n",
            "bert_model.ckpt.data-00000-of-00001  bert_model.ckpt.meta\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kfoWWwSt7-Ay",
        "colab_type": "text"
      },
      "source": [
        "#### Converting ckpt files to pytorch_model.bin"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hNHTiwNPxAPP",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 360
        },
        "outputId": "2df109c5-563f-40ff-ae6f-5dc1d5d90f54"
      },
      "source": [
        "!pip install transformers"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.6/dist-packages (2.11.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from transformers) (20.4)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.6/dist-packages (from transformers) (0.0.43)\n",
            "Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers) (0.7)\n",
            "Requirement already satisfied: tokenizers==0.7.0 in /usr/local/lib/python3.6/dist-packages (from transformers) (0.7.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.18.4)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.6/dist-packages (from transformers) (0.1.91)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.41.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from packaging->transformers) (1.12.0)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->transformers) (2.4.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (0.15.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2020.4.5.1)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.9)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JQwIx8jRtzsV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from transformers import BertConfig, BertForPreTraining, load_tf_weights_in_bert\n",
        "import torch"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4Rk_RpIzxHpo",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 323
        },
        "outputId": "5dd58232-7891-4f57-f28e-6211a5e010d8"
      },
      "source": [
        "config = BertConfig.from_json_file('./bert_config.json')\n",
        "print(\"Building PyTorch model from configuration: {}\".format(str(config)))\n",
        "model = BertForPreTraining(config)\n",
        "\n",
        "# Load weights from tf checkpoint\n",
        "load_tf_weights_in_bert(model, config, './bert_model.ckpt')\n",
        "\n",
        "# Save pytorch-model\n",
        "print(\"Save PyTorch model to {}\".format('./pytorch_model.bin'))\n",
        "torch.save(model.state_dict(), './pytorch_model.bin')"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Building PyTorch model from configuration: BertConfig {\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 1024,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 4096,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 16,\n",
            "  \"num_hidden_layers\": 24,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n",
            "Save PyTorch model to ./pytorch_model.bin\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fCmHkPcqx2YJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!rm bert_model.ckpt.data-00000-of-00001\n",
        "!rm bert_model.ckpt.index\n",
        "!rm bert_model.ckpt.meta"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rJTWI8IJs_Ae",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "os.chdir('../')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_D9ZuJlBxfCi",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 258
        },
        "outputId": "5f010317-d0f8-4407-cba5-284bb8b31fc7"
      },
      "source": [
        "!python train.py --data_dir data/task1/ --bert_model_dir model/ --model_dir experiments/base_model"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "device: cuda, n_gpu: 1, 16-bits training: False\n",
            "Loading the datasets...\n",
            "loading vocabulary file model/vocab.txt\n",
            "Traceback (most recent call last):\n",
            "  File \"train.py\", line 168, in <module>\n",
            "    train_data = data_loader.load_data('train')\n",
            "  File \"/content/BERT-keyphrase-extraction/data_loader.py\", line 83, in load_data\n",
            "    self.load_sentences_tags(sentences_file, tags_path, data)\n",
            "  File \"/content/BERT-keyphrase-extraction/data_loader.py\", line 51, in load_sentences_tags\n",
            "    sentences.append(self.tokenizer.convert_tokens_to_ids(tokens))\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/pytorch_pretrained_bert/tokenization.py\", line 121, in convert_tokens_to_ids\n",
            "    ids.append(self.vocab[token])\n",
            "KeyError: 'nonzero'\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "drpWiw6f1Dyg",
        "colab_type": "text"
      },
      "source": [
        "There is a difference in the vocabulary from the SciBERT models so, we use it instead for this dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "klAa_vyfxgHv",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "85f5f5de-067c-40b7-e4b2-b60acd572265"
      },
      "source": [
        "os.chdir('../')\n",
        "!tar -C BERT-keyphrase-extraction/model -xvf scibert_scivocab_uncased.tar"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "scibert_scivocab_uncased/\n",
            "scibert_scivocab_uncased/weights.tar.gz\n",
            "scibert_scivocab_uncased/vocab.txt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FsdRXz5-2IUe",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "2d397ad3-5c20-4ea7-d768-73b7da5bf7b5"
      },
      "source": [
        "os.chdir('BERT-keyphrase-extraction/model')\n",
        "!rm vocab.txt\n",
        "!mv ./scibert_scivocab_uncased/vocab.txt ./"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "rm: cannot remove 'scibert_scivocab_uncased': Is a directory\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z-ixN4lI2LmZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!rm -r scibert_scivocab_uncased"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V1Hyj6-A2h9g",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 972
        },
        "outputId": "25b3a444-7139-46f0-aa07-a47453d6dc91"
      },
      "source": [
        "os.chdir('../')\n",
        "!python train.py --data_dir data/task1/ --bert_model_dir model/ --model_dir experiments/base_model"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "device: cuda, n_gpu: 1, 16-bits training: False\n",
            "Loading the datasets...\n",
            "loading vocabulary file model/vocab.txt\n",
            "loading archive file model/\n",
            "Model config {\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 1024,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 4096,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"num_attention_heads\": 16,\n",
            "  \"num_hidden_layers\": 24,\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n",
            "Weights of BertForTokenClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']\n",
            "Weights from pretrained model not used in BertForTokenClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
            "Starting training for 20 epoch(s)\n",
            "Epoch 1/20\n",
            "/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
            "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n",
            " 32% 32/99 [00:34<01:16,  1.15s/it, loss=0.681]Traceback (most recent call last):\n",
            "  File \"train.py\", line 219, in <module>\n",
            "    train_and_evaluate(model, train_data, val_data, optimizer, scheduler, params, args.model_dir, args.restore_file)\n",
            "  File \"train.py\", line 98, in train_and_evaluate\n",
            "    train(model, train_data_iterator, optimizer, scheduler, params)\n",
            "  File \"train.py\", line 53, in train\n",
            "    loss = model(batch_data, token_type_ids=None, attention_mask=batch_masks, labels=batch_tags)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\", line 550, in __call__\n",
            "    result = self.forward(*input, **kwargs)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/pytorch_pretrained_bert/modeling.py\", line 1124, in forward\n",
            "    sequence_output, _ = self.bert(input_ids, token_type_ids, attention_mask, output_all_encoded_layers=False)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\", line 550, in __call__\n",
            "    result = self.forward(*input, **kwargs)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/pytorch_pretrained_bert/modeling.py\", line 733, in forward\n",
            "    output_all_encoded_layers=output_all_encoded_layers)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\", line 550, in __call__\n",
            "    result = self.forward(*input, **kwargs)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/pytorch_pretrained_bert/modeling.py\", line 406, in forward\n",
            "    hidden_states = layer_module(hidden_states, attention_mask)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\", line 550, in __call__\n",
            "    result = self.forward(*input, **kwargs)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/pytorch_pretrained_bert/modeling.py\", line 392, in forward\n",
            "    intermediate_output = self.intermediate(attention_output)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\", line 550, in __call__\n",
            "    result = self.forward(*input, **kwargs)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/pytorch_pretrained_bert/modeling.py\", line 365, in forward\n",
            "    hidden_states = self.intermediate_act_fn(hidden_states)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/pytorch_pretrained_bert/modeling.py\", line 124, in gelu\n",
            "    return x * 0.5 * (1.0 + torch.erf(x / math.sqrt(2.0)))\n",
            "RuntimeError: CUDA out of memory. Tried to allocate 22.00 MiB (GPU 0; 11.17 GiB total capacity; 10.36 GiB already allocated; 9.81 MiB free; 10.86 GiB reserved in total by PyTorch)\n",
            " 32% 32/99 [00:35<01:14,  1.11s/it, loss=0.681]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MU4C2WUq-DsT",
        "colab_type": "text"
      },
      "source": [
        "Now, we need to refer to use the recommended values of batch size of 4 and sequence length of 512, with 6 epochs, if GPU's VRAM is around 11 GB. (changed in experiments/base_model/params.json)\n",
        "But, even after reducing that we still get CUDA out of Memory error so we use batch size of 2 and sequence length of 512, with 50 epochs (`min_epoch_num:20` and `patience_num:10`). Learning rate = 3e-4 and weight decay = 0.01"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o2nN1PJV-m7l",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "412fbd32-22c1-4b42-dd83-5f17403b0756"
      },
      "source": [
        "!python evaluate.py --data_dir data/task1/ --bert_model_dir model/ --model_dir experiments/base_model --restore_file best"
      ],
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loading the dataset...\n",
            "loading vocabulary file model/vocab.txt\n",
            "- done.\n",
            "Starting evaluation...\n",
            "- Test metrics: loss: 00.61; f1: 04.03\n",
            "             precision    recall  f1-score   support\n",
            "\n",
            "          I       3.41      4.91      4.03       916\n",
            "\n",
            "avg / total       3.41      4.91      4.03       916\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pbioPIJ8Mn7q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}