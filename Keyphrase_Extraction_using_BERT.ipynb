{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Keyphrase Extraction using BERT.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyOnlPdr9PCZeJldEU1KnAeS",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AnshMittal1811/BERT/blob/master/Keyphrase_Extraction_using_BERT.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0d2Z4Gf8pPN6",
        "colab_type": "text"
      },
      "source": [
        "# Internship Assignment"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GTHGFf6MLaPq",
        "colab_type": "code",
        "outputId": "2b0f2131-2bb0-4649-c127-0c6faa080631",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "!ls"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "sample_data\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KgK6qHLxbzuI",
        "colab_type": "text"
      },
      "source": [
        "# Task 2\n",
        "Using the model trained and built using Pranav's code, please extract key phrases the text file."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1a8gwGLcV_Sf",
        "colab_type": "text"
      },
      "source": [
        "### Trying SciBERT according to the Instructions in BERT-keyphrase-extraction Repository"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yr2cfj1iLGB2",
        "colab_type": "code",
        "outputId": "6ee662f7-bf5c-4411-c1ca-a31a44e42e32",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "!git clone https://github.com/pranav-ust/BERT-keyphrase-extraction"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'BERT-keyphrase-extraction'...\n",
            "remote: Enumerating objects: 77, done.\u001b[K\n",
            "remote: Total 77 (delta 0), reused 0 (delta 0), pack-reused 77\u001b[K\n",
            "Unpacking objects: 100% (77/77), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8-LVVvkZLWZa",
        "colab_type": "code",
        "outputId": "d658f3e5-7134-435a-be2a-57aac692edbb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 411
        }
      },
      "source": [
        "!pip install pytorch-pretrained-BERT"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting pytorch-pretrained-BERT\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d7/e0/c08d5553b89973d9a240605b9c12404bcf8227590de62bae27acbcfe076b/pytorch_pretrained_bert-0.6.2-py3-none-any.whl (123kB)\n",
            "\r\u001b[K     |██▋                             | 10kB 19.8MB/s eta 0:00:01\r\u001b[K     |█████▎                          | 20kB 2.9MB/s eta 0:00:01\r\u001b[K     |████████                        | 30kB 3.8MB/s eta 0:00:01\r\u001b[K     |██████████▋                     | 40kB 4.2MB/s eta 0:00:01\r\u001b[K     |█████████████▎                  | 51kB 3.4MB/s eta 0:00:01\r\u001b[K     |███████████████▉                | 61kB 3.8MB/s eta 0:00:01\r\u001b[K     |██████████████████▌             | 71kB 4.0MB/s eta 0:00:01\r\u001b[K     |█████████████████████▏          | 81kB 4.4MB/s eta 0:00:01\r\u001b[K     |███████████████████████▉        | 92kB 4.7MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▌     | 102kB 4.6MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▏  | 112kB 4.6MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▊| 122kB 4.6MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 133kB 4.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (from pytorch-pretrained-BERT) (1.13.19)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from pytorch-pretrained-BERT) (4.41.1)\n",
            "Requirement already satisfied: torch>=0.4.1 in /usr/local/lib/python3.6/dist-packages (from pytorch-pretrained-BERT) (1.5.0+cu101)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.6/dist-packages (from pytorch-pretrained-BERT) (2019.12.20)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from pytorch-pretrained-BERT) (1.18.4)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from pytorch-pretrained-BERT) (2.23.0)\n",
            "Requirement already satisfied: botocore<1.17.0,>=1.16.19 in /usr/local/lib/python3.6/dist-packages (from boto3->pytorch-pretrained-BERT) (1.16.19)\n",
            "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->pytorch-pretrained-BERT) (0.10.0)\n",
            "Requirement already satisfied: s3transfer<0.4.0,>=0.3.0 in /usr/local/lib/python3.6/dist-packages (from boto3->pytorch-pretrained-BERT) (0.3.3)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torch>=0.4.1->pytorch-pretrained-BERT) (0.16.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch-pretrained-BERT) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch-pretrained-BERT) (2020.4.5.1)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch-pretrained-BERT) (2.9)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch-pretrained-BERT) (1.24.3)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.6/dist-packages (from botocore<1.17.0,>=1.16.19->boto3->pytorch-pretrained-BERT) (2.8.1)\n",
            "Requirement already satisfied: docutils<0.16,>=0.10 in /usr/local/lib/python3.6/dist-packages (from botocore<1.17.0,>=1.16.19->boto3->pytorch-pretrained-BERT) (0.15.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.6/dist-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.17.0,>=1.16.19->boto3->pytorch-pretrained-BERT) (1.12.0)\n",
            "Installing collected packages: pytorch-pretrained-BERT\n",
            "Successfully installed pytorch-pretrained-BERT-0.6.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nr0W1G5dXFme",
        "colab_type": "code",
        "outputId": "d6a40617-9b57-4e1f-8dcd-59775ec98b18",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 224
        }
      },
      "source": [
        "!wget https://s3-us-west-2.amazonaws.com/ai2-s2-research/scibert/pytorch_models/scibert_scivocab_uncased.tar"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-06-06 21:20:23--  https://s3-us-west-2.amazonaws.com/ai2-s2-research/scibert/pytorch_models/scibert_scivocab_uncased.tar\n",
            "Resolving s3-us-west-2.amazonaws.com (s3-us-west-2.amazonaws.com)... 52.218.184.48\n",
            "Connecting to s3-us-west-2.amazonaws.com (s3-us-west-2.amazonaws.com)|52.218.184.48|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 410593280 (392M) [application/x-tar]\n",
            "Saving to: ‘scibert_scivocab_uncased.tar’\n",
            "\n",
            "scibert_scivocab_un 100%[===================>] 391.57M  48.0MB/s    in 8.0s    \n",
            "\n",
            "2020-06-06 21:20:31 (48.8 MB/s) - ‘scibert_scivocab_uncased.tar’ saved [410593280/410593280]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mMn_J6TncWcr",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "e939355e-ca86-4ada-9a97-809d6d43b7e9"
      },
      "source": [
        "!ls"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "BERT-keyphrase-extraction  sample_data\tscibert_scivocab_uncased.tar\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uevzGz6iXb_Q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!mkdir BERT-keyphrase-extraction/model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cquYFXzzZMd2",
        "colab_type": "code",
        "outputId": "8d173067-5fcf-475f-8f26-b3e034e11297",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "!tar -C BERT-keyphrase-extraction/model -xvf scibert_scivocab_uncased.tar"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "scibert_scivocab_uncased/\n",
            "scibert_scivocab_uncased/weights.tar.gz\n",
            "scibert_scivocab_uncased/vocab.txt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5qbYnASwcnhI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "os.chdir('BERT-keyphrase-extraction/model/scibert_scivocab_uncased')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sMPlaiD2cwFT",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "82fc7940-391f-431d-b437-4778d013d2a0"
      },
      "source": [
        "!ls"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "vocab.txt  weights.tar.gz\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JRN96UJmeSkd",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "693424ad-0437-4ad6-b703-f9a064cd2ef1"
      },
      "source": [
        "!tar -C ./ -xvf weights.tar.gz"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "bert_config.json\n",
            "pytorch_model.bin\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6vLFCyGigPhC",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "7788fe30-e31a-4db3-ab22-a7e5ec824c7b"
      },
      "source": [
        "!ls"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "bert_config.json  pytorch_model.bin  vocab.txt\tweights.tar.gz\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BYsStO7-fimR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!mv pytorch_model.bin ../\n",
        "!mv vocab.txt ../\n",
        "!mv bert_config.json ../"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_Y--NAexhW55",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "os.chdir('../../')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wbFzBi7PhgBu",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "ccb13df0-7c7a-4e88-9494-e3b06014ffab"
      },
      "source": [
        "!ls"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "data\t\tevaluate.py  metrics.py  README.md  utils.py\n",
            "data_loader.py\texperiments  model\t train.py\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3gL9-TW_iy5J",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!rm -r model/scibert_scivocab_uncased"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zKOFy_ulcDWr",
        "colab_type": "text"
      },
      "source": [
        "Here, we change the `params.json` file in `experiments/base_model` to increase number of epochs to 50 along with `min_epoch_num : 25` and `patience_num : 10`. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-undX2ZEVw1c",
        "colab_type": "code",
        "outputId": "d44bcada-029d-4630-fcd7-057c80b4e5a1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "!python train.py --data_dir data/task1/ --bert_model_dir model/ --model_dir experiments/base_model"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "device: cuda, n_gpu: 1, 16-bits training: False\n",
            "Loading the datasets...\n",
            "loading vocabulary file model/vocab.txt\n",
            "loading archive file model/\n",
            "Model config {\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"vocab_size\": 31090\n",
            "}\n",
            "\n",
            "Weights of BertForTokenClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']\n",
            "Weights from pretrained model not used in BertForTokenClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
            "Starting training for 50 epoch(s)\n",
            "Epoch 1/50\n",
            "/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
            "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n",
            "100% 99/99 [00:13<00:00,  7.23it/s, loss=0.339]\n",
            "- Train metrics: loss: 00.17; f1: 46.43\n",
            "- Val metrics: loss: 00.29; f1: 34.42\n",
            "- Found new best F1\n",
            "Epoch 2/50\n",
            "100% 99/99 [00:13<00:00,  7.35it/s, loss=0.182]\n",
            "- Train metrics: loss: 00.08; f1: 58.81\n",
            "- Val metrics: loss: 00.30; f1: 39.06\n",
            "- Found new best F1\n",
            "Epoch 3/50\n",
            "100% 99/99 [00:13<00:00,  7.30it/s, loss=0.106]\n",
            "- Train metrics: loss: 00.04; f1: 67.12\n",
            "- Val metrics: loss: 00.29; f1: 42.79\n",
            "- Found new best F1\n",
            "Epoch 4/50\n",
            "100% 99/99 [00:13<00:00,  7.17it/s, loss=0.053]\n",
            "- Train metrics: loss: 00.04; f1: 67.35\n",
            "- Val metrics: loss: 00.49; f1: 38.40\n",
            "Epoch 5/50\n",
            "100% 99/99 [00:13<00:00,  7.27it/s, loss=0.036]\n",
            "- Train metrics: loss: 00.01; f1: 71.76\n",
            "- Val metrics: loss: 00.37; f1: 44.59\n",
            "- Found new best F1\n",
            "Epoch 6/50\n",
            "100% 99/99 [00:13<00:00,  7.28it/s, loss=0.023]\n",
            "- Train metrics: loss: 00.01; f1: 72.45\n",
            "- Val metrics: loss: 00.39; f1: 44.81\n",
            "- Found new best F1\n",
            "Epoch 7/50\n",
            "100% 99/99 [00:13<00:00,  7.31it/s, loss=0.015]\n",
            "- Train metrics: loss: 00.01; f1: 72.77\n",
            "- Val metrics: loss: 00.46; f1: 44.39\n",
            "Epoch 8/50\n",
            "100% 99/99 [00:13<00:00,  7.23it/s, loss=0.010]\n",
            "- Train metrics: loss: 00.00; f1: 73.57\n",
            "- Val metrics: loss: 00.53; f1: 42.70\n",
            "Epoch 9/50\n",
            "100% 99/99 [00:13<00:00,  7.17it/s, loss=0.006]\n",
            "- Train metrics: loss: 00.00; f1: 73.67\n",
            "- Val metrics: loss: 00.64; f1: 42.96\n",
            "Epoch 10/50\n",
            "100% 99/99 [00:13<00:00,  7.39it/s, loss=0.006]\n",
            "- Train metrics: loss: 00.00; f1: 73.55\n",
            "- Val metrics: loss: 00.52; f1: 45.34\n",
            "- Found new best F1\n",
            "Epoch 11/50\n",
            "100% 99/99 [00:13<00:00,  7.38it/s, loss=0.006]\n",
            "- Train metrics: loss: 00.00; f1: 73.46\n",
            "- Val metrics: loss: 00.52; f1: 44.86\n",
            "Epoch 12/50\n",
            "100% 99/99 [00:13<00:00,  7.18it/s, loss=0.004]\n",
            "- Train metrics: loss: 00.00; f1: 73.50\n",
            "- Val metrics: loss: 00.54; f1: 44.52\n",
            "Epoch 13/50\n",
            "100% 99/99 [00:13<00:00,  7.47it/s, loss=0.004]\n",
            "- Train metrics: loss: 00.00; f1: 73.22\n",
            "- Val metrics: loss: 00.49; f1: 45.05\n",
            "Epoch 14/50\n",
            "100% 99/99 [00:13<00:00,  7.20it/s, loss=0.003]\n",
            "- Train metrics: loss: 00.00; f1: 72.93\n",
            "- Val metrics: loss: 00.56; f1: 44.83\n",
            "Epoch 15/50\n",
            "100% 99/99 [00:13<00:00,  7.24it/s, loss=0.001]\n",
            "- Train metrics: loss: 00.00; f1: 73.32\n",
            "- Val metrics: loss: 00.65; f1: 44.82\n",
            "Epoch 16/50\n",
            "100% 99/99 [00:13<00:00,  7.32it/s, loss=0.002]\n",
            "- Train metrics: loss: 00.00; f1: 73.37\n",
            "- Val metrics: loss: 00.53; f1: 45.43\n",
            "- Found new best F1\n",
            "Epoch 17/50\n",
            "100% 99/99 [00:13<00:00,  7.32it/s, loss=0.002]\n",
            "- Train metrics: loss: 00.00; f1: 73.72\n",
            "- Val metrics: loss: 00.55; f1: 45.24\n",
            "Epoch 18/50\n",
            "100% 99/99 [00:13<00:00,  7.40it/s, loss=0.003]\n",
            "- Train metrics: loss: 00.00; f1: 73.93\n",
            "- Val metrics: loss: 00.54; f1: 44.61\n",
            "Epoch 19/50\n",
            "100% 99/99 [00:13<00:00,  7.16it/s, loss=0.001]\n",
            "- Train metrics: loss: 00.00; f1: 73.68\n",
            "- Val metrics: loss: 00.56; f1: 44.43\n",
            "Epoch 20/50\n",
            "100% 99/99 [00:13<00:00,  7.11it/s, loss=0.002]\n",
            "- Train metrics: loss: 00.00; f1: 73.41\n",
            "- Val metrics: loss: 00.53; f1: 45.27\n",
            "Epoch 21/50\n",
            "100% 99/99 [00:13<00:00,  7.22it/s, loss=0.001]\n",
            "- Train metrics: loss: 00.00; f1: 73.57\n",
            "- Val metrics: loss: 00.58; f1: 44.61\n",
            "Epoch 22/50\n",
            "100% 99/99 [00:13<00:00,  7.25it/s, loss=0.002]\n",
            "- Train metrics: loss: 00.00; f1: 73.22\n",
            "- Val metrics: loss: 00.51; f1: 45.44\n",
            "- Found new best F1\n",
            "Epoch 23/50\n",
            "100% 99/99 [00:13<00:00,  7.44it/s, loss=0.002]\n",
            "- Train metrics: loss: 00.00; f1: 73.14\n",
            "- Val metrics: loss: 00.50; f1: 44.91\n",
            "Epoch 24/50\n",
            "100% 99/99 [00:13<00:00,  7.09it/s, loss=0.002]\n",
            "- Train metrics: loss: 00.00; f1: 73.66\n",
            "- Val metrics: loss: 00.51; f1: 46.06\n",
            "- Found new best F1\n",
            "Epoch 25/50\n",
            "100% 99/99 [00:13<00:00,  7.32it/s, loss=0.001]\n",
            "- Train metrics: loss: 00.00; f1: 73.50\n",
            "- Val metrics: loss: 00.59; f1: 45.06\n",
            "Epoch 26/50\n",
            "100% 99/99 [00:13<00:00,  7.23it/s, loss=0.001]\n",
            "- Train metrics: loss: 00.00; f1: 73.43\n",
            "- Val metrics: loss: 00.58; f1: 45.49\n",
            "Epoch 27/50\n",
            "100% 99/99 [00:13<00:00,  7.25it/s, loss=0.001]\n",
            "- Train metrics: loss: 00.00; f1: 73.40\n",
            "- Val metrics: loss: 00.64; f1: 44.44\n",
            "Epoch 28/50\n",
            "100% 99/99 [00:13<00:00,  7.49it/s, loss=0.001]\n",
            "- Train metrics: loss: 00.00; f1: 73.72\n",
            "- Val metrics: loss: 00.65; f1: 44.53\n",
            "Epoch 29/50\n",
            "100% 99/99 [00:13<00:00,  7.32it/s, loss=0.000]\n",
            "- Train metrics: loss: 00.00; f1: 73.91\n",
            "- Val metrics: loss: 00.65; f1: 45.06\n",
            "Epoch 30/50\n",
            "100% 99/99 [00:13<00:00,  7.34it/s, loss=0.001]\n",
            "- Train metrics: loss: 00.00; f1: 73.33\n",
            "- Val metrics: loss: 00.63; f1: 46.45\n",
            "- Found new best F1\n",
            "Epoch 31/50\n",
            "100% 99/99 [00:13<00:00,  7.27it/s, loss=0.002]\n",
            "- Train metrics: loss: 00.00; f1: 73.20\n",
            "- Val metrics: loss: 00.68; f1: 44.34\n",
            "Epoch 32/50\n",
            "100% 99/99 [00:13<00:00,  7.27it/s, loss=0.000]\n",
            "- Train metrics: loss: 00.00; f1: 72.96\n",
            "- Val metrics: loss: 00.64; f1: 44.55\n",
            "Epoch 33/50\n",
            "100% 99/99 [00:13<00:00,  7.28it/s, loss=0.001]\n",
            "- Train metrics: loss: 00.00; f1: 73.50\n",
            "- Val metrics: loss: 00.66; f1: 45.49\n",
            "Epoch 34/50\n",
            "100% 99/99 [00:13<00:00,  7.19it/s, loss=0.000]\n",
            "- Train metrics: loss: 00.00; f1: 73.74\n",
            "- Val metrics: loss: 00.66; f1: 45.15\n",
            "Epoch 35/50\n",
            "100% 99/99 [00:13<00:00,  7.25it/s, loss=0.000]\n",
            "- Train metrics: loss: 00.00; f1: 73.93\n",
            "- Val metrics: loss: 00.70; f1: 44.95\n",
            "Epoch 36/50\n",
            "100% 99/99 [00:13<00:00,  7.26it/s, loss=0.001]\n",
            "- Train metrics: loss: 00.00; f1: 73.59\n",
            "- Val metrics: loss: 00.73; f1: 44.92\n",
            "Epoch 37/50\n",
            "100% 99/99 [00:13<00:00,  7.27it/s, loss=0.001]\n",
            "- Train metrics: loss: 00.00; f1: 73.84\n",
            "- Val metrics: loss: 00.69; f1: 45.05\n",
            "Epoch 38/50\n",
            "100% 99/99 [00:13<00:00,  7.20it/s, loss=0.000]\n",
            "- Train metrics: loss: 00.00; f1: 73.78\n",
            "- Val metrics: loss: 00.70; f1: 45.59\n",
            "Epoch 39/50\n",
            "100% 99/99 [00:13<00:00,  7.20it/s, loss=0.002]\n",
            "- Train metrics: loss: 00.00; f1: 73.72\n",
            "- Val metrics: loss: 00.70; f1: 44.94\n",
            "Epoch 40/50\n",
            "100% 99/99 [00:13<00:00,  7.38it/s, loss=0.000]\n",
            "- Train metrics: loss: 00.00; f1: 73.42\n",
            "- Val metrics: loss: 00.67; f1: 45.52\n",
            "Best val f1: 46.45\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y93-xNYyWe6l",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "60d03033-943a-4dd0-a451-72cda821e2f6"
      },
      "source": [
        "!python evaluate.py --data_dir data/task1/ --bert_model_dir model/ --model_dir experiments/base_model --restore_file best"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loading the dataset...\n",
            "loading vocabulary file model/vocab.txt\n",
            "- done.\n",
            "Starting evaluation...\n",
            "- Test metrics: loss: 00.63; f1: 46.45\n",
            "             precision    recall  f1-score   support\n",
            "\n",
            "          I      37.18     61.89     46.45       921\n",
            "\n",
            "avg / total      37.18     61.89     46.45       921\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VugYv4ZdmMrC",
        "colab_type": "text"
      },
      "source": [
        "Using trained and validated model to get keyphrases."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RRhPUKlxmKlv",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        },
        "outputId": "2f562c49-c4b1-4a48-a023-e9a5e2fe2868"
      },
      "source": [
        "!git clone https://github.com/AnshMittal1811/BERT"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'BERT'...\n",
            "remote: Enumerating objects: 9, done.\u001b[K\n",
            "remote: Counting objects:  11% (1/9)\u001b[K\rremote: Counting objects:  22% (2/9)\u001b[K\rremote: Counting objects:  33% (3/9)\u001b[K\rremote: Counting objects:  44% (4/9)\u001b[K\rremote: Counting objects:  55% (5/9)\u001b[K\rremote: Counting objects:  66% (6/9)\u001b[K\rremote: Counting objects:  77% (7/9)\u001b[K\rremote: Counting objects:  88% (8/9)\u001b[K\rremote: Counting objects: 100% (9/9)\u001b[K\rremote: Counting objects: 100% (9/9), done.\u001b[K\n",
            "remote: Compressing objects:  12% (1/8)\u001b[K\rremote: Compressing objects:  25% (2/8)\u001b[K\rremote: Compressing objects:  37% (3/8)\u001b[K\rremote: Compressing objects:  50% (4/8)\u001b[K\rremote: Compressing objects:  62% (5/8)\u001b[K\rremote: Compressing objects:  75% (6/8)\u001b[K\rremote: Compressing objects:  87% (7/8)\u001b[K\rremote: Compressing objects: 100% (8/8)\u001b[K\rremote: Compressing objects: 100% (8/8), done.\u001b[K\n",
            "Unpacking objects:  11% (1/9)   \rUnpacking objects:  22% (2/9)   \rUnpacking objects:  33% (3/9)   \rUnpacking objects:  44% (4/9)   \rUnpacking objects:  55% (5/9)   \rUnpacking objects:  66% (6/9)   \rremote: Total 9 (delta 1), reused 0 (delta 0), pack-reused 0\u001b[K\n",
            "Unpacking objects:  77% (7/9)   \rUnpacking objects:  88% (8/9)   \rUnpacking objects: 100% (9/9)   \rUnpacking objects: 100% (9/9), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aYXs6oyOpwDp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "os.chdir('./BERT')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8r4oIwhzrARf",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "a5e4975a-44e5-45b8-c897-2e17db040fc7"
      },
      "source": [
        "!ls"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "h1_7.txt  Keyphrase_Extraction_using_BERT.ipynb  README.md\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RZELNeHUrCU6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!mv h1_7.txt ../BERT-keyphrase-extraction/"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FKDbJ-nBrRhB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!rm -r BERT/"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_uWVgHawrf_F",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "os.chdir('./BERT-keyphrase-extraction')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gbs-3WtYrvD6",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "c77dfa8b-e53e-4e2a-91bd-6c90fd4ce943"
      },
      "source": [
        "!ls"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "data\t\tevaluate.py  h1_7.txt\t model\t      README.md  utils.py\n",
            "data_loader.py\texperiments  metrics.py  __pycache__  train.py\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mfihoPq3xiB2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "ddaa8c06-fbeb-46b9-cbe2-05667694a5f4"
      },
      "source": [
        "!mkdir ./Keyphrase-Extract\n",
        "!mv h1_7.txt ./Keyphrase-Extract\n",
        "!ls"
      ],
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "data\t\texperiments\t   model\ttrain.py\n",
            "data_loader.py\tKeyphrase-Extract  __pycache__\tutils.py\n",
            "evaluate.py\tmetrics.py\t   README.md\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U3nGwtdjtYz1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from pytorch_pretrained_bert import BertForTokenClassification, BertConfig, BertTokenizer\n",
        "import random\n",
        "import logging\n",
        "import os\n",
        "from metrics import f1_score\n",
        "from metrics import classification_report\n",
        "import numpy as np\n",
        "import torch\n",
        "from data_loader import DataLoader\n",
        "import utils\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "61xNOrm10jG1",
        "colab_type": "text"
      },
      "source": [
        "### Prepare Dataset\n",
        "Convert Semeval 2010 keyword recognition dataset to IO format dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d3a8D_Ta0t7U",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "from nltk.tokenize import sent_tokenize"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FOm-SctE4Elb",
        "colab_type": "text"
      },
      "source": [
        "We need to convert the file such that we can create a `tags.txt` file, similar to what has been created for `./data/task1/train`, `./data/task1/val`, `./data/task1/test`, and `./data/task1`. But, the facility for Semeval had been removed in AllenAI so we aren't able to calculate the IO format for the given text files without the archives."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "97I_zdzq1yCy",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 513
        },
        "outputId": "27ddd15e-7dcd-43e3-dd81-dbe735b926ff"
      },
      "source": [
        "# Load the parameters from json file\n",
        "json_path = './experiments/base_model/params.json'\n",
        "assert os.path.isfile(json_path), \"No json configuration file found at {}\".format(json_path)\n",
        "params = utils.Params(json_path)\n",
        "\n",
        "# Use GPUs if available\n",
        "params.device = torch.device('cuda' if torch.cuda.is_available() \n",
        "                                    else 'cpu')\n",
        "params.n_gpu = torch.cuda.device_count()\n",
        "params.multi_gpu = False\n",
        "\n",
        "# Set the random seed for reproducible experiments\n",
        "random.seed(23)\n",
        "torch.manual_seed(23)\n",
        "if params.n_gpu > 0:\n",
        "    torch.cuda.manual_seed_all(23)  # set random seed for all GPUs\n",
        "params.seed = 23\n",
        "\n",
        "# Initialize the DataLoader (Problem) <----------- Problem over here (need IO formatted tags)\n",
        "data_loader = DataLoader('./Keyphrase-Extract', './model', params, token_pad_idx=0)\n",
        "\n",
        "# Load data \n",
        "test_data = data_loader.load_data('test')\n",
        "\n",
        "# Specify the test set size\n",
        "params.test_size = test_data['size']\n",
        "params.eval_steps = params.test_size // params.batch_size\n",
        "test_data_iterator = data_loader.data_iterator(test_data, shuffle=False)\n",
        "\n",
        "config_path = './model/bert_config.json'\n",
        "config = BertConfig.from_json_file(config_path)\n",
        "model = BertForTokenClassification(config, num_labels=len(tag2idx))\n",
        "\n",
        "model.to(params.device)\n",
        "\n",
        "utils.load_checkpoint('experiments/base_model/best.pth.tar', model)\n",
        "\n",
        "\n",
        "# Opening a file and reading a file\n",
        "f = open(\"demofile.txt\", \"r\")\n",
        "input = []\n",
        "\n",
        "for x in f:\n",
        "  input.append(x)\n",
        "\n",
        "# keyphrases extracted if the code works well\n",
        "s = model(input)"
      ],
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-76-bb97e64f7494>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;31m# Initialize the DataLoader\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m \u001b[0mdata_loader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDataLoader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'./Keyphrase-Extract'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'./model'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtoken_pad_idx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;31m# Load data (Problem) ---\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/BERT-keyphrase-extraction/data_loader.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, data_dir, bert_model_dir, params, token_pad_idx)\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoken_pad_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m         \u001b[0mtags\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_tags\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtag2idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mtag\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtag\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtags\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0midx2tag\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtag\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtag\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtags\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/BERT-keyphrase-extraction/data_loader.py\u001b[0m in \u001b[0;36mload_tags\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     33\u001b[0m         \u001b[0mtags\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[0mfile_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'tags.txt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m         \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mtag\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m                 \u001b[0mtags\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtag\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './Keyphrase-Extract/tags.txt'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NDd3wn8Xpf1u",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!rm -r model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C5ZCxGsbpsWv",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "ce3efa36-52c9-4d94-94c3-0774fa7b6d18"
      },
      "source": [
        "!ls"
      ],
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "data\t\tevaluate.py  Keyphrase-Extract\t__pycache__  train.py\n",
            "data_loader.py\texperiments  metrics.py\t\tREADME.md    utils.py\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zS_Uf3_NykEh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "os.chdir('./experiments/base_model')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cRWsqOoqyr6n",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!rm ./best.pth.tar\n",
        "!rm ./evaluate.log\n",
        "!rm ./last.pth.tar\n",
        "!rm ./train.log"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kaXpFSLkbj3Q",
        "colab_type": "text"
      },
      "source": [
        "# Task 1 \n",
        "\n",
        "Use the code in\n",
        "https://github.com/pranav-ust/BERT-keyphrase-extraction to train it\n",
        "with default parameters and evaluate the model for just Subtask 1 with\n",
        "the following change: Instead of using SciBERT, please use BERT. As\n",
        "you can see, Pranav has given evaluation results with SciBERT as F1\n",
        "score: 0.6259; Precision: 0.5986; Recall: 0.6558; Support: 921. Please\n",
        "give the results using BERT instead."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hiwgG7mlpYHP",
        "colab_type": "text"
      },
      "source": [
        "### Trying BERT according using BERT-keyphrase-extraction Repository"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cSaJcPtammue",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "os.chdir('../../')\n",
        "!mkdir ./model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AFQPHh4rpz43",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 224
        },
        "outputId": "3ca85d61-4915-4254-f9bc-2926918d7c2b"
      },
      "source": [
        "!wget https://storage.googleapis.com/bert_models/2018_10_18/uncased_L-24_H-1024_A-16.zip"
      ],
      "execution_count": 82,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-06-06 23:11:57--  https://storage.googleapis.com/bert_models/2018_10_18/uncased_L-24_H-1024_A-16.zip\n",
            "Resolving storage.googleapis.com (storage.googleapis.com)... 74.125.20.128, 2607:f8b0:400e:c00::80\n",
            "Connecting to storage.googleapis.com (storage.googleapis.com)|74.125.20.128|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1247797031 (1.2G) [application/zip]\n",
            "Saving to: ‘uncased_L-24_H-1024_A-16.zip’\n",
            "\n",
            "uncased_L-24_H-1024 100%[===================>]   1.16G   110MB/s    in 11s     \n",
            "\n",
            "2020-06-06 23:12:08 (111 MB/s) - ‘uncased_L-24_H-1024_A-16.zip’ saved [1247797031/1247797031]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5Em1ZBmCq3k5",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        },
        "outputId": "8164211c-242b-4788-a93b-d7f597a6d4cd"
      },
      "source": [
        "!unzip uncased_L-24_H-1024_A-16.zip -d ./model"
      ],
      "execution_count": 83,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Archive:  uncased_L-24_H-1024_A-16.zip\n",
            "   creating: ./model/uncased_L-24_H-1024_A-16/\n",
            "  inflating: ./model/uncased_L-24_H-1024_A-16/bert_model.ckpt.meta  \n",
            "  inflating: ./model/uncased_L-24_H-1024_A-16/bert_model.ckpt.data-00000-of-00001  \n",
            "  inflating: ./model/uncased_L-24_H-1024_A-16/vocab.txt  \n",
            "  inflating: ./model/uncased_L-24_H-1024_A-16/bert_model.ckpt.index  \n",
            "  inflating: ./model/uncased_L-24_H-1024_A-16/bert_config.json  \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WJRR-bPBvQuV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!rm -r uncased_L-24_H-1024_A-16.zip"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0NmD78UKvqNS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "os.chdir('./model/uncased_L-24_H-1024_A-16')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GrzRnELovvul",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!mv bert_model.ckpt.meta ../\n",
        "!mv bert_model.ckpt.data-00000-of-00001 ../\n",
        "!mv bert_config.json ../\n",
        "!mv vocab.txt ../\n",
        "!mv bert_model.ckpt.index ../"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "egXN7_dtvvcv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!rm -r ../uncased_L-24_H-1024_A-16"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y0tKgDcavvZp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "os.chdir('/content/BERT-keyphrase-extraction/model')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rk46t8XFwj16",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "318b0c5d-2f4d-4821-e554-d7fa3d7d664a"
      },
      "source": [
        "!ls"
      ],
      "execution_count": 89,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "bert_config.json\t\t     bert_model.ckpt.index  vocab.txt\n",
            "bert_model.ckpt.data-00000-of-00001  bert_model.ckpt.meta\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kfoWWwSt7-Ay",
        "colab_type": "text"
      },
      "source": [
        "#### Converting ckpt files to pytorch_model.bin"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hNHTiwNPxAPP",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 615
        },
        "outputId": "c1e88b5a-0edc-46fb-a263-42f5f612859a"
      },
      "source": [
        "!pip install transformers"
      ],
      "execution_count": 90,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting transformers\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/48/35/ad2c5b1b8f99feaaf9d7cdadaeef261f098c6e1a6a2935d4d07662a6b780/transformers-2.11.0-py3-none-any.whl (674kB)\n",
            "\u001b[K     |████████████████████████████████| 675kB 4.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n",
            "Collecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7d/34/09d19aff26edcc8eb2a01bed8e98f13a1537005d31e95233fd48216eed10/sacremoses-0.0.43.tar.gz (883kB)\n",
            "\u001b[K     |████████████████████████████████| 890kB 11.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n",
            "Collecting sentencepiece\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d4/a4/d0a884c4300004a78cca907a6ff9a5e9fe4f090f5d95ab341c53d28cbc58/sentencepiece-0.1.91-cp36-cp36m-manylinux1_x86_64.whl (1.1MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1MB 30.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from transformers) (20.4)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.23.0)\n",
            "Collecting tokenizers==0.7.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/14/e5/a26eb4716523808bb0a799fcfdceb6ebf77a18169d9591b2f46a9adb87d9/tokenizers-0.7.0-cp36-cp36m-manylinux1_x86_64.whl (3.8MB)\n",
            "\u001b[K     |████████████████████████████████| 3.8MB 40.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.18.4)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.41.1)\n",
            "Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers) (0.7)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (1.12.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (0.15.1)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->transformers) (2.4.7)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.9)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2020.4.5.1)\n",
            "Building wheels for collected packages: sacremoses\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.43-cp36-none-any.whl size=893260 sha256=83af3bc4c24f2e5ebd5d119c376f2f862ac78293396b68f4910302f9314e7b42\n",
            "  Stored in directory: /root/.cache/pip/wheels/29/3c/fd/7ce5c3f0666dab31a50123635e6fb5e19ceb42ce38d4e58f45\n",
            "Successfully built sacremoses\n",
            "Installing collected packages: sacremoses, sentencepiece, tokenizers, transformers\n",
            "Successfully installed sacremoses-0.0.43 sentencepiece-0.1.91 tokenizers-0.7.0 transformers-2.11.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JQwIx8jRtzsV",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "d2ccd53d-6d08-40ec-8d3a-62f75a028df5"
      },
      "source": [
        "from transformers import BertConfig, BertForPreTraining, load_tf_weights_in_bert\n",
        "import torch"
      ],
      "execution_count": 91,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "PyTorch version 1.5.0+cu101 available.\n",
            "TensorFlow version 2.2.0 available.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4Rk_RpIzxHpo",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "679e5c4b-168d-41d7-c9c1-ffc54edb1980"
      },
      "source": [
        "config = BertConfig.from_json_file('./bert_config.json')\n",
        "print(\"Building PyTorch model from configuration: {}\".format(str(config)))\n",
        "model = BertForPreTraining(config)\n",
        "\n",
        "# Load weights from tf checkpoint\n",
        "load_tf_weights_in_bert(model, config, './bert_model.ckpt')\n",
        "\n",
        "# Save pytorch-model\n",
        "print(\"Save PyTorch model to {}\".format('./pytorch_model.bin'))\n",
        "torch.save(model.state_dict(), './pytorch_model.bin')"
      ],
      "execution_count": 92,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Building PyTorch model from configuration: BertConfig {\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 1024,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 4096,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 16,\n",
            "  \"num_hidden_layers\": 24,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Converting TensorFlow checkpoint from /content/BERT-keyphrase-extraction/model/bert_model.ckpt\n",
            "Loading TF weight bert/embeddings/LayerNorm/beta with shape [1024]\n",
            "Loading TF weight bert/embeddings/LayerNorm/gamma with shape [1024]\n",
            "Loading TF weight bert/embeddings/position_embeddings with shape [512, 1024]\n",
            "Loading TF weight bert/embeddings/token_type_embeddings with shape [2, 1024]\n",
            "Loading TF weight bert/embeddings/word_embeddings with shape [30522, 1024]\n",
            "Loading TF weight bert/encoder/layer_0/attention/output/LayerNorm/beta with shape [1024]\n",
            "Loading TF weight bert/encoder/layer_0/attention/output/LayerNorm/gamma with shape [1024]\n",
            "Loading TF weight bert/encoder/layer_0/attention/output/dense/bias with shape [1024]\n",
            "Loading TF weight bert/encoder/layer_0/attention/output/dense/kernel with shape [1024, 1024]\n",
            "Loading TF weight bert/encoder/layer_0/attention/self/key/bias with shape [1024]\n",
            "Loading TF weight bert/encoder/layer_0/attention/self/key/kernel with shape [1024, 1024]\n",
            "Loading TF weight bert/encoder/layer_0/attention/self/query/bias with shape [1024]\n",
            "Loading TF weight bert/encoder/layer_0/attention/self/query/kernel with shape [1024, 1024]\n",
            "Loading TF weight bert/encoder/layer_0/attention/self/value/bias with shape [1024]\n",
            "Loading TF weight bert/encoder/layer_0/attention/self/value/kernel with shape [1024, 1024]\n",
            "Loading TF weight bert/encoder/layer_0/intermediate/dense/bias with shape [4096]\n",
            "Loading TF weight bert/encoder/layer_0/intermediate/dense/kernel with shape [1024, 4096]\n",
            "Loading TF weight bert/encoder/layer_0/output/LayerNorm/beta with shape [1024]\n",
            "Loading TF weight bert/encoder/layer_0/output/LayerNorm/gamma with shape [1024]\n",
            "Loading TF weight bert/encoder/layer_0/output/dense/bias with shape [1024]\n",
            "Loading TF weight bert/encoder/layer_0/output/dense/kernel with shape [4096, 1024]\n",
            "Loading TF weight bert/encoder/layer_1/attention/output/LayerNorm/beta with shape [1024]\n",
            "Loading TF weight bert/encoder/layer_1/attention/output/LayerNorm/gamma with shape [1024]\n",
            "Loading TF weight bert/encoder/layer_1/attention/output/dense/bias with shape [1024]\n",
            "Loading TF weight bert/encoder/layer_1/attention/output/dense/kernel with shape [1024, 1024]\n",
            "Loading TF weight bert/encoder/layer_1/attention/self/key/bias with shape [1024]\n",
            "Loading TF weight bert/encoder/layer_1/attention/self/key/kernel with shape [1024, 1024]\n",
            "Loading TF weight bert/encoder/layer_1/attention/self/query/bias with shape [1024]\n",
            "Loading TF weight bert/encoder/layer_1/attention/self/query/kernel with shape [1024, 1024]\n",
            "Loading TF weight bert/encoder/layer_1/attention/self/value/bias with shape [1024]\n",
            "Loading TF weight bert/encoder/layer_1/attention/self/value/kernel with shape [1024, 1024]\n",
            "Loading TF weight bert/encoder/layer_1/intermediate/dense/bias with shape [4096]\n",
            "Loading TF weight bert/encoder/layer_1/intermediate/dense/kernel with shape [1024, 4096]\n",
            "Loading TF weight bert/encoder/layer_1/output/LayerNorm/beta with shape [1024]\n",
            "Loading TF weight bert/encoder/layer_1/output/LayerNorm/gamma with shape [1024]\n",
            "Loading TF weight bert/encoder/layer_1/output/dense/bias with shape [1024]\n",
            "Loading TF weight bert/encoder/layer_1/output/dense/kernel with shape [4096, 1024]\n",
            "Loading TF weight bert/encoder/layer_10/attention/output/LayerNorm/beta with shape [1024]\n",
            "Loading TF weight bert/encoder/layer_10/attention/output/LayerNorm/gamma with shape [1024]\n",
            "Loading TF weight bert/encoder/layer_10/attention/output/dense/bias with shape [1024]\n",
            "Loading TF weight bert/encoder/layer_10/attention/output/dense/kernel with shape [1024, 1024]\n",
            "Loading TF weight bert/encoder/layer_10/attention/self/key/bias with shape [1024]\n",
            "Loading TF weight bert/encoder/layer_10/attention/self/key/kernel with shape [1024, 1024]\n",
            "Loading TF weight bert/encoder/layer_10/attention/self/query/bias with shape [1024]\n",
            "Loading TF weight bert/encoder/layer_10/attention/self/query/kernel with shape [1024, 1024]\n",
            "Loading TF weight bert/encoder/layer_10/attention/self/value/bias with shape [1024]\n",
            "Loading TF weight bert/encoder/layer_10/attention/self/value/kernel with shape [1024, 1024]\n",
            "Loading TF weight bert/encoder/layer_10/intermediate/dense/bias with shape [4096]\n",
            "Loading TF weight bert/encoder/layer_10/intermediate/dense/kernel with shape [1024, 4096]\n",
            "Loading TF weight bert/encoder/layer_10/output/LayerNorm/beta with shape [1024]\n",
            "Loading TF weight bert/encoder/layer_10/output/LayerNorm/gamma with shape [1024]\n",
            "Loading TF weight bert/encoder/layer_10/output/dense/bias with shape [1024]\n",
            "Loading TF weight bert/encoder/layer_10/output/dense/kernel with shape [4096, 1024]\n",
            "Loading TF weight bert/encoder/layer_11/attention/output/LayerNorm/beta with shape [1024]\n",
            "Loading TF weight bert/encoder/layer_11/attention/output/LayerNorm/gamma with shape [1024]\n",
            "Loading TF weight bert/encoder/layer_11/attention/output/dense/bias with shape [1024]\n",
            "Loading TF weight bert/encoder/layer_11/attention/output/dense/kernel with shape [1024, 1024]\n",
            "Loading TF weight bert/encoder/layer_11/attention/self/key/bias with shape [1024]\n",
            "Loading TF weight bert/encoder/layer_11/attention/self/key/kernel with shape [1024, 1024]\n",
            "Loading TF weight bert/encoder/layer_11/attention/self/query/bias with shape [1024]\n",
            "Loading TF weight bert/encoder/layer_11/attention/self/query/kernel with shape [1024, 1024]\n",
            "Loading TF weight bert/encoder/layer_11/attention/self/value/bias with shape [1024]\n",
            "Loading TF weight bert/encoder/layer_11/attention/self/value/kernel with shape [1024, 1024]\n",
            "Loading TF weight bert/encoder/layer_11/intermediate/dense/bias with shape [4096]\n",
            "Loading TF weight bert/encoder/layer_11/intermediate/dense/kernel with shape [1024, 4096]\n",
            "Loading TF weight bert/encoder/layer_11/output/LayerNorm/beta with shape [1024]\n",
            "Loading TF weight bert/encoder/layer_11/output/LayerNorm/gamma with shape [1024]\n",
            "Loading TF weight bert/encoder/layer_11/output/dense/bias with shape [1024]\n",
            "Loading TF weight bert/encoder/layer_11/output/dense/kernel with shape [4096, 1024]\n",
            "Loading TF weight bert/encoder/layer_12/attention/output/LayerNorm/beta with shape [1024]\n",
            "Loading TF weight bert/encoder/layer_12/attention/output/LayerNorm/gamma with shape [1024]\n",
            "Loading TF weight bert/encoder/layer_12/attention/output/dense/bias with shape [1024]\n",
            "Loading TF weight bert/encoder/layer_12/attention/output/dense/kernel with shape [1024, 1024]\n",
            "Loading TF weight bert/encoder/layer_12/attention/self/key/bias with shape [1024]\n",
            "Loading TF weight bert/encoder/layer_12/attention/self/key/kernel with shape [1024, 1024]\n",
            "Loading TF weight bert/encoder/layer_12/attention/self/query/bias with shape [1024]\n",
            "Loading TF weight bert/encoder/layer_12/attention/self/query/kernel with shape [1024, 1024]\n",
            "Loading TF weight bert/encoder/layer_12/attention/self/value/bias with shape [1024]\n",
            "Loading TF weight bert/encoder/layer_12/attention/self/value/kernel with shape [1024, 1024]\n",
            "Loading TF weight bert/encoder/layer_12/intermediate/dense/bias with shape [4096]\n",
            "Loading TF weight bert/encoder/layer_12/intermediate/dense/kernel with shape [1024, 4096]\n",
            "Loading TF weight bert/encoder/layer_12/output/LayerNorm/beta with shape [1024]\n",
            "Loading TF weight bert/encoder/layer_12/output/LayerNorm/gamma with shape [1024]\n",
            "Loading TF weight bert/encoder/layer_12/output/dense/bias with shape [1024]\n",
            "Loading TF weight bert/encoder/layer_12/output/dense/kernel with shape [4096, 1024]\n",
            "Loading TF weight bert/encoder/layer_13/attention/output/LayerNorm/beta with shape [1024]\n",
            "Loading TF weight bert/encoder/layer_13/attention/output/LayerNorm/gamma with shape [1024]\n",
            "Loading TF weight bert/encoder/layer_13/attention/output/dense/bias with shape [1024]\n",
            "Loading TF weight bert/encoder/layer_13/attention/output/dense/kernel with shape [1024, 1024]\n",
            "Loading TF weight bert/encoder/layer_13/attention/self/key/bias with shape [1024]\n",
            "Loading TF weight bert/encoder/layer_13/attention/self/key/kernel with shape [1024, 1024]\n",
            "Loading TF weight bert/encoder/layer_13/attention/self/query/bias with shape [1024]\n",
            "Loading TF weight bert/encoder/layer_13/attention/self/query/kernel with shape [1024, 1024]\n",
            "Loading TF weight bert/encoder/layer_13/attention/self/value/bias with shape [1024]\n",
            "Loading TF weight bert/encoder/layer_13/attention/self/value/kernel with shape [1024, 1024]\n",
            "Loading TF weight bert/encoder/layer_13/intermediate/dense/bias with shape [4096]\n",
            "Loading TF weight bert/encoder/layer_13/intermediate/dense/kernel with shape [1024, 4096]\n",
            "Loading TF weight bert/encoder/layer_13/output/LayerNorm/beta with shape [1024]\n",
            "Loading TF weight bert/encoder/layer_13/output/LayerNorm/gamma with shape [1024]\n",
            "Loading TF weight bert/encoder/layer_13/output/dense/bias with shape [1024]\n",
            "Loading TF weight bert/encoder/layer_13/output/dense/kernel with shape [4096, 1024]\n",
            "Loading TF weight bert/encoder/layer_14/attention/output/LayerNorm/beta with shape [1024]\n",
            "Loading TF weight bert/encoder/layer_14/attention/output/LayerNorm/gamma with shape [1024]\n",
            "Loading TF weight bert/encoder/layer_14/attention/output/dense/bias with shape [1024]\n",
            "Loading TF weight bert/encoder/layer_14/attention/output/dense/kernel with shape [1024, 1024]\n",
            "Loading TF weight bert/encoder/layer_14/attention/self/key/bias with shape [1024]\n",
            "Loading TF weight bert/encoder/layer_14/attention/self/key/kernel with shape [1024, 1024]\n",
            "Loading TF weight bert/encoder/layer_14/attention/self/query/bias with shape [1024]\n",
            "Loading TF weight bert/encoder/layer_14/attention/self/query/kernel with shape [1024, 1024]\n",
            "Loading TF weight bert/encoder/layer_14/attention/self/value/bias with shape [1024]\n",
            "Loading TF weight bert/encoder/layer_14/attention/self/value/kernel with shape [1024, 1024]\n",
            "Loading TF weight bert/encoder/layer_14/intermediate/dense/bias with shape [4096]\n",
            "Loading TF weight bert/encoder/layer_14/intermediate/dense/kernel with shape [1024, 4096]\n",
            "Loading TF weight bert/encoder/layer_14/output/LayerNorm/beta with shape [1024]\n",
            "Loading TF weight bert/encoder/layer_14/output/LayerNorm/gamma with shape [1024]\n",
            "Loading TF weight bert/encoder/layer_14/output/dense/bias with shape [1024]\n",
            "Loading TF weight bert/encoder/layer_14/output/dense/kernel with shape [4096, 1024]\n",
            "Loading TF weight bert/encoder/layer_15/attention/output/LayerNorm/beta with shape [1024]\n",
            "Loading TF weight bert/encoder/layer_15/attention/output/LayerNorm/gamma with shape [1024]\n",
            "Loading TF weight bert/encoder/layer_15/attention/output/dense/bias with shape [1024]\n",
            "Loading TF weight bert/encoder/layer_15/attention/output/dense/kernel with shape [1024, 1024]\n",
            "Loading TF weight bert/encoder/layer_15/attention/self/key/bias with shape [1024]\n",
            "Loading TF weight bert/encoder/layer_15/attention/self/key/kernel with shape [1024, 1024]\n",
            "Loading TF weight bert/encoder/layer_15/attention/self/query/bias with shape [1024]\n",
            "Loading TF weight bert/encoder/layer_15/attention/self/query/kernel with shape [1024, 1024]\n",
            "Loading TF weight bert/encoder/layer_15/attention/self/value/bias with shape [1024]\n",
            "Loading TF weight bert/encoder/layer_15/attention/self/value/kernel with shape [1024, 1024]\n",
            "Loading TF weight bert/encoder/layer_15/intermediate/dense/bias with shape [4096]\n",
            "Loading TF weight bert/encoder/layer_15/intermediate/dense/kernel with shape [1024, 4096]\n",
            "Loading TF weight bert/encoder/layer_15/output/LayerNorm/beta with shape [1024]\n",
            "Loading TF weight bert/encoder/layer_15/output/LayerNorm/gamma with shape [1024]\n",
            "Loading TF weight bert/encoder/layer_15/output/dense/bias with shape [1024]\n",
            "Loading TF weight bert/encoder/layer_15/output/dense/kernel with shape [4096, 1024]\n",
            "Loading TF weight bert/encoder/layer_16/attention/output/LayerNorm/beta with shape [1024]\n",
            "Loading TF weight bert/encoder/layer_16/attention/output/LayerNorm/gamma with shape [1024]\n",
            "Loading TF weight bert/encoder/layer_16/attention/output/dense/bias with shape [1024]\n",
            "Loading TF weight bert/encoder/layer_16/attention/output/dense/kernel with shape [1024, 1024]\n",
            "Loading TF weight bert/encoder/layer_16/attention/self/key/bias with shape [1024]\n",
            "Loading TF weight bert/encoder/layer_16/attention/self/key/kernel with shape [1024, 1024]\n",
            "Loading TF weight bert/encoder/layer_16/attention/self/query/bias with shape [1024]\n",
            "Loading TF weight bert/encoder/layer_16/attention/self/query/kernel with shape [1024, 1024]\n",
            "Loading TF weight bert/encoder/layer_16/attention/self/value/bias with shape [1024]\n",
            "Loading TF weight bert/encoder/layer_16/attention/self/value/kernel with shape [1024, 1024]\n",
            "Loading TF weight bert/encoder/layer_16/intermediate/dense/bias with shape [4096]\n",
            "Loading TF weight bert/encoder/layer_16/intermediate/dense/kernel with shape [1024, 4096]\n",
            "Loading TF weight bert/encoder/layer_16/output/LayerNorm/beta with shape [1024]\n",
            "Loading TF weight bert/encoder/layer_16/output/LayerNorm/gamma with shape [1024]\n",
            "Loading TF weight bert/encoder/layer_16/output/dense/bias with shape [1024]\n",
            "Loading TF weight bert/encoder/layer_16/output/dense/kernel with shape [4096, 1024]\n",
            "Loading TF weight bert/encoder/layer_17/attention/output/LayerNorm/beta with shape [1024]\n",
            "Loading TF weight bert/encoder/layer_17/attention/output/LayerNorm/gamma with shape [1024]\n",
            "Loading TF weight bert/encoder/layer_17/attention/output/dense/bias with shape [1024]\n",
            "Loading TF weight bert/encoder/layer_17/attention/output/dense/kernel with shape [1024, 1024]\n",
            "Loading TF weight bert/encoder/layer_17/attention/self/key/bias with shape [1024]\n",
            "Loading TF weight bert/encoder/layer_17/attention/self/key/kernel with shape [1024, 1024]\n",
            "Loading TF weight bert/encoder/layer_17/attention/self/query/bias with shape [1024]\n",
            "Loading TF weight bert/encoder/layer_17/attention/self/query/kernel with shape [1024, 1024]\n",
            "Loading TF weight bert/encoder/layer_17/attention/self/value/bias with shape [1024]\n",
            "Loading TF weight bert/encoder/layer_17/attention/self/value/kernel with shape [1024, 1024]\n",
            "Loading TF weight bert/encoder/layer_17/intermediate/dense/bias with shape [4096]\n",
            "Loading TF weight bert/encoder/layer_17/intermediate/dense/kernel with shape [1024, 4096]\n",
            "Loading TF weight bert/encoder/layer_17/output/LayerNorm/beta with shape [1024]\n",
            "Loading TF weight bert/encoder/layer_17/output/LayerNorm/gamma with shape [1024]\n",
            "Loading TF weight bert/encoder/layer_17/output/dense/bias with shape [1024]\n",
            "Loading TF weight bert/encoder/layer_17/output/dense/kernel with shape [4096, 1024]\n",
            "Loading TF weight bert/encoder/layer_18/attention/output/LayerNorm/beta with shape [1024]\n",
            "Loading TF weight bert/encoder/layer_18/attention/output/LayerNorm/gamma with shape [1024]\n",
            "Loading TF weight bert/encoder/layer_18/attention/output/dense/bias with shape [1024]\n",
            "Loading TF weight bert/encoder/layer_18/attention/output/dense/kernel with shape [1024, 1024]\n",
            "Loading TF weight bert/encoder/layer_18/attention/self/key/bias with shape [1024]\n",
            "Loading TF weight bert/encoder/layer_18/attention/self/key/kernel with shape [1024, 1024]\n",
            "Loading TF weight bert/encoder/layer_18/attention/self/query/bias with shape [1024]\n",
            "Loading TF weight bert/encoder/layer_18/attention/self/query/kernel with shape [1024, 1024]\n",
            "Loading TF weight bert/encoder/layer_18/attention/self/value/bias with shape [1024]\n",
            "Loading TF weight bert/encoder/layer_18/attention/self/value/kernel with shape [1024, 1024]\n",
            "Loading TF weight bert/encoder/layer_18/intermediate/dense/bias with shape [4096]\n",
            "Loading TF weight bert/encoder/layer_18/intermediate/dense/kernel with shape [1024, 4096]\n",
            "Loading TF weight bert/encoder/layer_18/output/LayerNorm/beta with shape [1024]\n",
            "Loading TF weight bert/encoder/layer_18/output/LayerNorm/gamma with shape [1024]\n",
            "Loading TF weight bert/encoder/layer_18/output/dense/bias with shape [1024]\n",
            "Loading TF weight bert/encoder/layer_18/output/dense/kernel with shape [4096, 1024]\n",
            "Loading TF weight bert/encoder/layer_19/attention/output/LayerNorm/beta with shape [1024]\n",
            "Loading TF weight bert/encoder/layer_19/attention/output/LayerNorm/gamma with shape [1024]\n",
            "Loading TF weight bert/encoder/layer_19/attention/output/dense/bias with shape [1024]\n",
            "Loading TF weight bert/encoder/layer_19/attention/output/dense/kernel with shape [1024, 1024]\n",
            "Loading TF weight bert/encoder/layer_19/attention/self/key/bias with shape [1024]\n",
            "Loading TF weight bert/encoder/layer_19/attention/self/key/kernel with shape [1024, 1024]\n",
            "Loading TF weight bert/encoder/layer_19/attention/self/query/bias with shape [1024]\n",
            "Loading TF weight bert/encoder/layer_19/attention/self/query/kernel with shape [1024, 1024]\n",
            "Loading TF weight bert/encoder/layer_19/attention/self/value/bias with shape [1024]\n",
            "Loading TF weight bert/encoder/layer_19/attention/self/value/kernel with shape [1024, 1024]\n",
            "Loading TF weight bert/encoder/layer_19/intermediate/dense/bias with shape [4096]\n",
            "Loading TF weight bert/encoder/layer_19/intermediate/dense/kernel with shape [1024, 4096]\n",
            "Loading TF weight bert/encoder/layer_19/output/LayerNorm/beta with shape [1024]\n",
            "Loading TF weight bert/encoder/layer_19/output/LayerNorm/gamma with shape [1024]\n",
            "Loading TF weight bert/encoder/layer_19/output/dense/bias with shape [1024]\n",
            "Loading TF weight bert/encoder/layer_19/output/dense/kernel with shape [4096, 1024]\n",
            "Loading TF weight bert/encoder/layer_2/attention/output/LayerNorm/beta with shape [1024]\n",
            "Loading TF weight bert/encoder/layer_2/attention/output/LayerNorm/gamma with shape [1024]\n",
            "Loading TF weight bert/encoder/layer_2/attention/output/dense/bias with shape [1024]\n",
            "Loading TF weight bert/encoder/layer_2/attention/output/dense/kernel with shape [1024, 1024]\n",
            "Loading TF weight bert/encoder/layer_2/attention/self/key/bias with shape [1024]\n",
            "Loading TF weight bert/encoder/layer_2/attention/self/key/kernel with shape [1024, 1024]\n",
            "Loading TF weight bert/encoder/layer_2/attention/self/query/bias with shape [1024]\n",
            "Loading TF weight bert/encoder/layer_2/attention/self/query/kernel with shape [1024, 1024]\n",
            "Loading TF weight bert/encoder/layer_2/attention/self/value/bias with shape [1024]\n",
            "Loading TF weight bert/encoder/layer_2/attention/self/value/kernel with shape [1024, 1024]\n",
            "Loading TF weight bert/encoder/layer_2/intermediate/dense/bias with shape [4096]\n",
            "Loading TF weight bert/encoder/layer_2/intermediate/dense/kernel with shape [1024, 4096]\n",
            "Loading TF weight bert/encoder/layer_2/output/LayerNorm/beta with shape [1024]\n",
            "Loading TF weight bert/encoder/layer_2/output/LayerNorm/gamma with shape [1024]\n",
            "Loading TF weight bert/encoder/layer_2/output/dense/bias with shape [1024]\n",
            "Loading TF weight bert/encoder/layer_2/output/dense/kernel with shape [4096, 1024]\n",
            "Loading TF weight bert/encoder/layer_20/attention/output/LayerNorm/beta with shape [1024]\n",
            "Loading TF weight bert/encoder/layer_20/attention/output/LayerNorm/gamma with shape [1024]\n",
            "Loading TF weight bert/encoder/layer_20/attention/output/dense/bias with shape [1024]\n",
            "Loading TF weight bert/encoder/layer_20/attention/output/dense/kernel with shape [1024, 1024]\n",
            "Loading TF weight bert/encoder/layer_20/attention/self/key/bias with shape [1024]\n",
            "Loading TF weight bert/encoder/layer_20/attention/self/key/kernel with shape [1024, 1024]\n",
            "Loading TF weight bert/encoder/layer_20/attention/self/query/bias with shape [1024]\n",
            "Loading TF weight bert/encoder/layer_20/attention/self/query/kernel with shape [1024, 1024]\n",
            "Loading TF weight bert/encoder/layer_20/attention/self/value/bias with shape [1024]\n",
            "Loading TF weight bert/encoder/layer_20/attention/self/value/kernel with shape [1024, 1024]\n",
            "Loading TF weight bert/encoder/layer_20/intermediate/dense/bias with shape [4096]\n",
            "Loading TF weight bert/encoder/layer_20/intermediate/dense/kernel with shape [1024, 4096]\n",
            "Loading TF weight bert/encoder/layer_20/output/LayerNorm/beta with shape [1024]\n",
            "Loading TF weight bert/encoder/layer_20/output/LayerNorm/gamma with shape [1024]\n",
            "Loading TF weight bert/encoder/layer_20/output/dense/bias with shape [1024]\n",
            "Loading TF weight bert/encoder/layer_20/output/dense/kernel with shape [4096, 1024]\n",
            "Loading TF weight bert/encoder/layer_21/attention/output/LayerNorm/beta with shape [1024]\n",
            "Loading TF weight bert/encoder/layer_21/attention/output/LayerNorm/gamma with shape [1024]\n",
            "Loading TF weight bert/encoder/layer_21/attention/output/dense/bias with shape [1024]\n",
            "Loading TF weight bert/encoder/layer_21/attention/output/dense/kernel with shape [1024, 1024]\n",
            "Loading TF weight bert/encoder/layer_21/attention/self/key/bias with shape [1024]\n",
            "Loading TF weight bert/encoder/layer_21/attention/self/key/kernel with shape [1024, 1024]\n",
            "Loading TF weight bert/encoder/layer_21/attention/self/query/bias with shape [1024]\n",
            "Loading TF weight bert/encoder/layer_21/attention/self/query/kernel with shape [1024, 1024]\n",
            "Loading TF weight bert/encoder/layer_21/attention/self/value/bias with shape [1024]\n",
            "Loading TF weight bert/encoder/layer_21/attention/self/value/kernel with shape [1024, 1024]\n",
            "Loading TF weight bert/encoder/layer_21/intermediate/dense/bias with shape [4096]\n",
            "Loading TF weight bert/encoder/layer_21/intermediate/dense/kernel with shape [1024, 4096]\n",
            "Loading TF weight bert/encoder/layer_21/output/LayerNorm/beta with shape [1024]\n",
            "Loading TF weight bert/encoder/layer_21/output/LayerNorm/gamma with shape [1024]\n",
            "Loading TF weight bert/encoder/layer_21/output/dense/bias with shape [1024]\n",
            "Loading TF weight bert/encoder/layer_21/output/dense/kernel with shape [4096, 1024]\n",
            "Loading TF weight bert/encoder/layer_22/attention/output/LayerNorm/beta with shape [1024]\n",
            "Loading TF weight bert/encoder/layer_22/attention/output/LayerNorm/gamma with shape [1024]\n",
            "Loading TF weight bert/encoder/layer_22/attention/output/dense/bias with shape [1024]\n",
            "Loading TF weight bert/encoder/layer_22/attention/output/dense/kernel with shape [1024, 1024]\n",
            "Loading TF weight bert/encoder/layer_22/attention/self/key/bias with shape [1024]\n",
            "Loading TF weight bert/encoder/layer_22/attention/self/key/kernel with shape [1024, 1024]\n",
            "Loading TF weight bert/encoder/layer_22/attention/self/query/bias with shape [1024]\n",
            "Loading TF weight bert/encoder/layer_22/attention/self/query/kernel with shape [1024, 1024]\n",
            "Loading TF weight bert/encoder/layer_22/attention/self/value/bias with shape [1024]\n",
            "Loading TF weight bert/encoder/layer_22/attention/self/value/kernel with shape [1024, 1024]\n",
            "Loading TF weight bert/encoder/layer_22/intermediate/dense/bias with shape [4096]\n",
            "Loading TF weight bert/encoder/layer_22/intermediate/dense/kernel with shape [1024, 4096]\n",
            "Loading TF weight bert/encoder/layer_22/output/LayerNorm/beta with shape [1024]\n",
            "Loading TF weight bert/encoder/layer_22/output/LayerNorm/gamma with shape [1024]\n",
            "Loading TF weight bert/encoder/layer_22/output/dense/bias with shape [1024]\n",
            "Loading TF weight bert/encoder/layer_22/output/dense/kernel with shape [4096, 1024]\n",
            "Loading TF weight bert/encoder/layer_23/attention/output/LayerNorm/beta with shape [1024]\n",
            "Loading TF weight bert/encoder/layer_23/attention/output/LayerNorm/gamma with shape [1024]\n",
            "Loading TF weight bert/encoder/layer_23/attention/output/dense/bias with shape [1024]\n",
            "Loading TF weight bert/encoder/layer_23/attention/output/dense/kernel with shape [1024, 1024]\n",
            "Loading TF weight bert/encoder/layer_23/attention/self/key/bias with shape [1024]\n",
            "Loading TF weight bert/encoder/layer_23/attention/self/key/kernel with shape [1024, 1024]\n",
            "Loading TF weight bert/encoder/layer_23/attention/self/query/bias with shape [1024]\n",
            "Loading TF weight bert/encoder/layer_23/attention/self/query/kernel with shape [1024, 1024]\n",
            "Loading TF weight bert/encoder/layer_23/attention/self/value/bias with shape [1024]\n",
            "Loading TF weight bert/encoder/layer_23/attention/self/value/kernel with shape [1024, 1024]\n",
            "Loading TF weight bert/encoder/layer_23/intermediate/dense/bias with shape [4096]\n",
            "Loading TF weight bert/encoder/layer_23/intermediate/dense/kernel with shape [1024, 4096]\n",
            "Loading TF weight bert/encoder/layer_23/output/LayerNorm/beta with shape [1024]\n",
            "Loading TF weight bert/encoder/layer_23/output/LayerNorm/gamma with shape [1024]\n",
            "Loading TF weight bert/encoder/layer_23/output/dense/bias with shape [1024]\n",
            "Loading TF weight bert/encoder/layer_23/output/dense/kernel with shape [4096, 1024]\n",
            "Loading TF weight bert/encoder/layer_3/attention/output/LayerNorm/beta with shape [1024]\n",
            "Loading TF weight bert/encoder/layer_3/attention/output/LayerNorm/gamma with shape [1024]\n",
            "Loading TF weight bert/encoder/layer_3/attention/output/dense/bias with shape [1024]\n",
            "Loading TF weight bert/encoder/layer_3/attention/output/dense/kernel with shape [1024, 1024]\n",
            "Loading TF weight bert/encoder/layer_3/attention/self/key/bias with shape [1024]\n",
            "Loading TF weight bert/encoder/layer_3/attention/self/key/kernel with shape [1024, 1024]\n",
            "Loading TF weight bert/encoder/layer_3/attention/self/query/bias with shape [1024]\n",
            "Loading TF weight bert/encoder/layer_3/attention/self/query/kernel with shape [1024, 1024]\n",
            "Loading TF weight bert/encoder/layer_3/attention/self/value/bias with shape [1024]\n",
            "Loading TF weight bert/encoder/layer_3/attention/self/value/kernel with shape [1024, 1024]\n",
            "Loading TF weight bert/encoder/layer_3/intermediate/dense/bias with shape [4096]\n",
            "Loading TF weight bert/encoder/layer_3/intermediate/dense/kernel with shape [1024, 4096]\n",
            "Loading TF weight bert/encoder/layer_3/output/LayerNorm/beta with shape [1024]\n",
            "Loading TF weight bert/encoder/layer_3/output/LayerNorm/gamma with shape [1024]\n",
            "Loading TF weight bert/encoder/layer_3/output/dense/bias with shape [1024]\n",
            "Loading TF weight bert/encoder/layer_3/output/dense/kernel with shape [4096, 1024]\n",
            "Loading TF weight bert/encoder/layer_4/attention/output/LayerNorm/beta with shape [1024]\n",
            "Loading TF weight bert/encoder/layer_4/attention/output/LayerNorm/gamma with shape [1024]\n",
            "Loading TF weight bert/encoder/layer_4/attention/output/dense/bias with shape [1024]\n",
            "Loading TF weight bert/encoder/layer_4/attention/output/dense/kernel with shape [1024, 1024]\n",
            "Loading TF weight bert/encoder/layer_4/attention/self/key/bias with shape [1024]\n",
            "Loading TF weight bert/encoder/layer_4/attention/self/key/kernel with shape [1024, 1024]\n",
            "Loading TF weight bert/encoder/layer_4/attention/self/query/bias with shape [1024]\n",
            "Loading TF weight bert/encoder/layer_4/attention/self/query/kernel with shape [1024, 1024]\n",
            "Loading TF weight bert/encoder/layer_4/attention/self/value/bias with shape [1024]\n",
            "Loading TF weight bert/encoder/layer_4/attention/self/value/kernel with shape [1024, 1024]\n",
            "Loading TF weight bert/encoder/layer_4/intermediate/dense/bias with shape [4096]\n",
            "Loading TF weight bert/encoder/layer_4/intermediate/dense/kernel with shape [1024, 4096]\n",
            "Loading TF weight bert/encoder/layer_4/output/LayerNorm/beta with shape [1024]\n",
            "Loading TF weight bert/encoder/layer_4/output/LayerNorm/gamma with shape [1024]\n",
            "Loading TF weight bert/encoder/layer_4/output/dense/bias with shape [1024]\n",
            "Loading TF weight bert/encoder/layer_4/output/dense/kernel with shape [4096, 1024]\n",
            "Loading TF weight bert/encoder/layer_5/attention/output/LayerNorm/beta with shape [1024]\n",
            "Loading TF weight bert/encoder/layer_5/attention/output/LayerNorm/gamma with shape [1024]\n",
            "Loading TF weight bert/encoder/layer_5/attention/output/dense/bias with shape [1024]\n",
            "Loading TF weight bert/encoder/layer_5/attention/output/dense/kernel with shape [1024, 1024]\n",
            "Loading TF weight bert/encoder/layer_5/attention/self/key/bias with shape [1024]\n",
            "Loading TF weight bert/encoder/layer_5/attention/self/key/kernel with shape [1024, 1024]\n",
            "Loading TF weight bert/encoder/layer_5/attention/self/query/bias with shape [1024]\n",
            "Loading TF weight bert/encoder/layer_5/attention/self/query/kernel with shape [1024, 1024]\n",
            "Loading TF weight bert/encoder/layer_5/attention/self/value/bias with shape [1024]\n",
            "Loading TF weight bert/encoder/layer_5/attention/self/value/kernel with shape [1024, 1024]\n",
            "Loading TF weight bert/encoder/layer_5/intermediate/dense/bias with shape [4096]\n",
            "Loading TF weight bert/encoder/layer_5/intermediate/dense/kernel with shape [1024, 4096]\n",
            "Loading TF weight bert/encoder/layer_5/output/LayerNorm/beta with shape [1024]\n",
            "Loading TF weight bert/encoder/layer_5/output/LayerNorm/gamma with shape [1024]\n",
            "Loading TF weight bert/encoder/layer_5/output/dense/bias with shape [1024]\n",
            "Loading TF weight bert/encoder/layer_5/output/dense/kernel with shape [4096, 1024]\n",
            "Loading TF weight bert/encoder/layer_6/attention/output/LayerNorm/beta with shape [1024]\n",
            "Loading TF weight bert/encoder/layer_6/attention/output/LayerNorm/gamma with shape [1024]\n",
            "Loading TF weight bert/encoder/layer_6/attention/output/dense/bias with shape [1024]\n",
            "Loading TF weight bert/encoder/layer_6/attention/output/dense/kernel with shape [1024, 1024]\n",
            "Loading TF weight bert/encoder/layer_6/attention/self/key/bias with shape [1024]\n",
            "Loading TF weight bert/encoder/layer_6/attention/self/key/kernel with shape [1024, 1024]\n",
            "Loading TF weight bert/encoder/layer_6/attention/self/query/bias with shape [1024]\n",
            "Loading TF weight bert/encoder/layer_6/attention/self/query/kernel with shape [1024, 1024]\n",
            "Loading TF weight bert/encoder/layer_6/attention/self/value/bias with shape [1024]\n",
            "Loading TF weight bert/encoder/layer_6/attention/self/value/kernel with shape [1024, 1024]\n",
            "Loading TF weight bert/encoder/layer_6/intermediate/dense/bias with shape [4096]\n",
            "Loading TF weight bert/encoder/layer_6/intermediate/dense/kernel with shape [1024, 4096]\n",
            "Loading TF weight bert/encoder/layer_6/output/LayerNorm/beta with shape [1024]\n",
            "Loading TF weight bert/encoder/layer_6/output/LayerNorm/gamma with shape [1024]\n",
            "Loading TF weight bert/encoder/layer_6/output/dense/bias with shape [1024]\n",
            "Loading TF weight bert/encoder/layer_6/output/dense/kernel with shape [4096, 1024]\n",
            "Loading TF weight bert/encoder/layer_7/attention/output/LayerNorm/beta with shape [1024]\n",
            "Loading TF weight bert/encoder/layer_7/attention/output/LayerNorm/gamma with shape [1024]\n",
            "Loading TF weight bert/encoder/layer_7/attention/output/dense/bias with shape [1024]\n",
            "Loading TF weight bert/encoder/layer_7/attention/output/dense/kernel with shape [1024, 1024]\n",
            "Loading TF weight bert/encoder/layer_7/attention/self/key/bias with shape [1024]\n",
            "Loading TF weight bert/encoder/layer_7/attention/self/key/kernel with shape [1024, 1024]\n",
            "Loading TF weight bert/encoder/layer_7/attention/self/query/bias with shape [1024]\n",
            "Loading TF weight bert/encoder/layer_7/attention/self/query/kernel with shape [1024, 1024]\n",
            "Loading TF weight bert/encoder/layer_7/attention/self/value/bias with shape [1024]\n",
            "Loading TF weight bert/encoder/layer_7/attention/self/value/kernel with shape [1024, 1024]\n",
            "Loading TF weight bert/encoder/layer_7/intermediate/dense/bias with shape [4096]\n",
            "Loading TF weight bert/encoder/layer_7/intermediate/dense/kernel with shape [1024, 4096]\n",
            "Loading TF weight bert/encoder/layer_7/output/LayerNorm/beta with shape [1024]\n",
            "Loading TF weight bert/encoder/layer_7/output/LayerNorm/gamma with shape [1024]\n",
            "Loading TF weight bert/encoder/layer_7/output/dense/bias with shape [1024]\n",
            "Loading TF weight bert/encoder/layer_7/output/dense/kernel with shape [4096, 1024]\n",
            "Loading TF weight bert/encoder/layer_8/attention/output/LayerNorm/beta with shape [1024]\n",
            "Loading TF weight bert/encoder/layer_8/attention/output/LayerNorm/gamma with shape [1024]\n",
            "Loading TF weight bert/encoder/layer_8/attention/output/dense/bias with shape [1024]\n",
            "Loading TF weight bert/encoder/layer_8/attention/output/dense/kernel with shape [1024, 1024]\n",
            "Loading TF weight bert/encoder/layer_8/attention/self/key/bias with shape [1024]\n",
            "Loading TF weight bert/encoder/layer_8/attention/self/key/kernel with shape [1024, 1024]\n",
            "Loading TF weight bert/encoder/layer_8/attention/self/query/bias with shape [1024]\n",
            "Loading TF weight bert/encoder/layer_8/attention/self/query/kernel with shape [1024, 1024]\n",
            "Loading TF weight bert/encoder/layer_8/attention/self/value/bias with shape [1024]\n",
            "Loading TF weight bert/encoder/layer_8/attention/self/value/kernel with shape [1024, 1024]\n",
            "Loading TF weight bert/encoder/layer_8/intermediate/dense/bias with shape [4096]\n",
            "Loading TF weight bert/encoder/layer_8/intermediate/dense/kernel with shape [1024, 4096]\n",
            "Loading TF weight bert/encoder/layer_8/output/LayerNorm/beta with shape [1024]\n",
            "Loading TF weight bert/encoder/layer_8/output/LayerNorm/gamma with shape [1024]\n",
            "Loading TF weight bert/encoder/layer_8/output/dense/bias with shape [1024]\n",
            "Loading TF weight bert/encoder/layer_8/output/dense/kernel with shape [4096, 1024]\n",
            "Loading TF weight bert/encoder/layer_9/attention/output/LayerNorm/beta with shape [1024]\n",
            "Loading TF weight bert/encoder/layer_9/attention/output/LayerNorm/gamma with shape [1024]\n",
            "Loading TF weight bert/encoder/layer_9/attention/output/dense/bias with shape [1024]\n",
            "Loading TF weight bert/encoder/layer_9/attention/output/dense/kernel with shape [1024, 1024]\n",
            "Loading TF weight bert/encoder/layer_9/attention/self/key/bias with shape [1024]\n",
            "Loading TF weight bert/encoder/layer_9/attention/self/key/kernel with shape [1024, 1024]\n",
            "Loading TF weight bert/encoder/layer_9/attention/self/query/bias with shape [1024]\n",
            "Loading TF weight bert/encoder/layer_9/attention/self/query/kernel with shape [1024, 1024]\n",
            "Loading TF weight bert/encoder/layer_9/attention/self/value/bias with shape [1024]\n",
            "Loading TF weight bert/encoder/layer_9/attention/self/value/kernel with shape [1024, 1024]\n",
            "Loading TF weight bert/encoder/layer_9/intermediate/dense/bias with shape [4096]\n",
            "Loading TF weight bert/encoder/layer_9/intermediate/dense/kernel with shape [1024, 4096]\n",
            "Loading TF weight bert/encoder/layer_9/output/LayerNorm/beta with shape [1024]\n",
            "Loading TF weight bert/encoder/layer_9/output/LayerNorm/gamma with shape [1024]\n",
            "Loading TF weight bert/encoder/layer_9/output/dense/bias with shape [1024]\n",
            "Loading TF weight bert/encoder/layer_9/output/dense/kernel with shape [4096, 1024]\n",
            "Loading TF weight bert/pooler/dense/bias with shape [1024]\n",
            "Loading TF weight bert/pooler/dense/kernel with shape [1024, 1024]\n",
            "Loading TF weight cls/predictions/output_bias with shape [30522]\n",
            "Loading TF weight cls/predictions/transform/LayerNorm/beta with shape [1024]\n",
            "Loading TF weight cls/predictions/transform/LayerNorm/gamma with shape [1024]\n",
            "Loading TF weight cls/predictions/transform/dense/bias with shape [1024]\n",
            "Loading TF weight cls/predictions/transform/dense/kernel with shape [1024, 1024]\n",
            "Loading TF weight cls/seq_relationship/output_bias with shape [2]\n",
            "Loading TF weight cls/seq_relationship/output_weights with shape [2, 1024]\n",
            "Initialize PyTorch weight ['bert', 'embeddings', 'LayerNorm', 'beta']\n",
            "Initialize PyTorch weight ['bert', 'embeddings', 'LayerNorm', 'gamma']\n",
            "Initialize PyTorch weight ['bert', 'embeddings', 'position_embeddings']\n",
            "Initialize PyTorch weight ['bert', 'embeddings', 'token_type_embeddings']\n",
            "Initialize PyTorch weight ['bert', 'embeddings', 'word_embeddings']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_0', 'attention', 'output', 'LayerNorm', 'beta']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_0', 'attention', 'output', 'LayerNorm', 'gamma']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_0', 'attention', 'output', 'dense', 'bias']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_0', 'attention', 'output', 'dense', 'kernel']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_0', 'attention', 'self', 'key', 'bias']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_0', 'attention', 'self', 'key', 'kernel']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_0', 'attention', 'self', 'query', 'bias']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_0', 'attention', 'self', 'query', 'kernel']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_0', 'attention', 'self', 'value', 'bias']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_0', 'attention', 'self', 'value', 'kernel']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_0', 'intermediate', 'dense', 'bias']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_0', 'intermediate', 'dense', 'kernel']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_0', 'output', 'LayerNorm', 'beta']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_0', 'output', 'LayerNorm', 'gamma']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_0', 'output', 'dense', 'bias']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_0', 'output', 'dense', 'kernel']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_1', 'attention', 'output', 'LayerNorm', 'beta']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_1', 'attention', 'output', 'LayerNorm', 'gamma']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_1', 'attention', 'output', 'dense', 'bias']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_1', 'attention', 'output', 'dense', 'kernel']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_1', 'attention', 'self', 'key', 'bias']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_1', 'attention', 'self', 'key', 'kernel']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_1', 'attention', 'self', 'query', 'bias']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_1', 'attention', 'self', 'query', 'kernel']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_1', 'attention', 'self', 'value', 'bias']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_1', 'attention', 'self', 'value', 'kernel']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_1', 'intermediate', 'dense', 'bias']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_1', 'intermediate', 'dense', 'kernel']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_1', 'output', 'LayerNorm', 'beta']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_1', 'output', 'LayerNorm', 'gamma']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_1', 'output', 'dense', 'bias']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_1', 'output', 'dense', 'kernel']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_10', 'attention', 'output', 'LayerNorm', 'beta']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_10', 'attention', 'output', 'LayerNorm', 'gamma']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_10', 'attention', 'output', 'dense', 'bias']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_10', 'attention', 'output', 'dense', 'kernel']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_10', 'attention', 'self', 'key', 'bias']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_10', 'attention', 'self', 'key', 'kernel']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_10', 'attention', 'self', 'query', 'bias']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_10', 'attention', 'self', 'query', 'kernel']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_10', 'attention', 'self', 'value', 'bias']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_10', 'attention', 'self', 'value', 'kernel']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_10', 'intermediate', 'dense', 'bias']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_10', 'intermediate', 'dense', 'kernel']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_10', 'output', 'LayerNorm', 'beta']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_10', 'output', 'LayerNorm', 'gamma']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_10', 'output', 'dense', 'bias']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_10', 'output', 'dense', 'kernel']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_11', 'attention', 'output', 'LayerNorm', 'beta']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_11', 'attention', 'output', 'LayerNorm', 'gamma']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_11', 'attention', 'output', 'dense', 'bias']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_11', 'attention', 'output', 'dense', 'kernel']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_11', 'attention', 'self', 'key', 'bias']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_11', 'attention', 'self', 'key', 'kernel']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_11', 'attention', 'self', 'query', 'bias']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_11', 'attention', 'self', 'query', 'kernel']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_11', 'attention', 'self', 'value', 'bias']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_11', 'attention', 'self', 'value', 'kernel']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_11', 'intermediate', 'dense', 'bias']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_11', 'intermediate', 'dense', 'kernel']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_11', 'output', 'LayerNorm', 'beta']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_11', 'output', 'LayerNorm', 'gamma']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_11', 'output', 'dense', 'bias']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_11', 'output', 'dense', 'kernel']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_12', 'attention', 'output', 'LayerNorm', 'beta']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_12', 'attention', 'output', 'LayerNorm', 'gamma']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_12', 'attention', 'output', 'dense', 'bias']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_12', 'attention', 'output', 'dense', 'kernel']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_12', 'attention', 'self', 'key', 'bias']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_12', 'attention', 'self', 'key', 'kernel']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_12', 'attention', 'self', 'query', 'bias']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_12', 'attention', 'self', 'query', 'kernel']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_12', 'attention', 'self', 'value', 'bias']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_12', 'attention', 'self', 'value', 'kernel']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_12', 'intermediate', 'dense', 'bias']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_12', 'intermediate', 'dense', 'kernel']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_12', 'output', 'LayerNorm', 'beta']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_12', 'output', 'LayerNorm', 'gamma']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_12', 'output', 'dense', 'bias']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_12', 'output', 'dense', 'kernel']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_13', 'attention', 'output', 'LayerNorm', 'beta']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_13', 'attention', 'output', 'LayerNorm', 'gamma']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_13', 'attention', 'output', 'dense', 'bias']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_13', 'attention', 'output', 'dense', 'kernel']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_13', 'attention', 'self', 'key', 'bias']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_13', 'attention', 'self', 'key', 'kernel']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_13', 'attention', 'self', 'query', 'bias']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_13', 'attention', 'self', 'query', 'kernel']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_13', 'attention', 'self', 'value', 'bias']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_13', 'attention', 'self', 'value', 'kernel']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_13', 'intermediate', 'dense', 'bias']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_13', 'intermediate', 'dense', 'kernel']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_13', 'output', 'LayerNorm', 'beta']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_13', 'output', 'LayerNorm', 'gamma']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_13', 'output', 'dense', 'bias']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_13', 'output', 'dense', 'kernel']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_14', 'attention', 'output', 'LayerNorm', 'beta']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_14', 'attention', 'output', 'LayerNorm', 'gamma']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_14', 'attention', 'output', 'dense', 'bias']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_14', 'attention', 'output', 'dense', 'kernel']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_14', 'attention', 'self', 'key', 'bias']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_14', 'attention', 'self', 'key', 'kernel']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_14', 'attention', 'self', 'query', 'bias']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_14', 'attention', 'self', 'query', 'kernel']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_14', 'attention', 'self', 'value', 'bias']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_14', 'attention', 'self', 'value', 'kernel']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_14', 'intermediate', 'dense', 'bias']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_14', 'intermediate', 'dense', 'kernel']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_14', 'output', 'LayerNorm', 'beta']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_14', 'output', 'LayerNorm', 'gamma']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_14', 'output', 'dense', 'bias']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_14', 'output', 'dense', 'kernel']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_15', 'attention', 'output', 'LayerNorm', 'beta']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_15', 'attention', 'output', 'LayerNorm', 'gamma']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_15', 'attention', 'output', 'dense', 'bias']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_15', 'attention', 'output', 'dense', 'kernel']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_15', 'attention', 'self', 'key', 'bias']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_15', 'attention', 'self', 'key', 'kernel']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_15', 'attention', 'self', 'query', 'bias']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_15', 'attention', 'self', 'query', 'kernel']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_15', 'attention', 'self', 'value', 'bias']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_15', 'attention', 'self', 'value', 'kernel']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_15', 'intermediate', 'dense', 'bias']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_15', 'intermediate', 'dense', 'kernel']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_15', 'output', 'LayerNorm', 'beta']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_15', 'output', 'LayerNorm', 'gamma']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_15', 'output', 'dense', 'bias']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_15', 'output', 'dense', 'kernel']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_16', 'attention', 'output', 'LayerNorm', 'beta']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_16', 'attention', 'output', 'LayerNorm', 'gamma']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_16', 'attention', 'output', 'dense', 'bias']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_16', 'attention', 'output', 'dense', 'kernel']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_16', 'attention', 'self', 'key', 'bias']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_16', 'attention', 'self', 'key', 'kernel']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_16', 'attention', 'self', 'query', 'bias']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_16', 'attention', 'self', 'query', 'kernel']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_16', 'attention', 'self', 'value', 'bias']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_16', 'attention', 'self', 'value', 'kernel']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_16', 'intermediate', 'dense', 'bias']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_16', 'intermediate', 'dense', 'kernel']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_16', 'output', 'LayerNorm', 'beta']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_16', 'output', 'LayerNorm', 'gamma']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_16', 'output', 'dense', 'bias']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_16', 'output', 'dense', 'kernel']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_17', 'attention', 'output', 'LayerNorm', 'beta']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_17', 'attention', 'output', 'LayerNorm', 'gamma']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_17', 'attention', 'output', 'dense', 'bias']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_17', 'attention', 'output', 'dense', 'kernel']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_17', 'attention', 'self', 'key', 'bias']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_17', 'attention', 'self', 'key', 'kernel']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_17', 'attention', 'self', 'query', 'bias']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_17', 'attention', 'self', 'query', 'kernel']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_17', 'attention', 'self', 'value', 'bias']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_17', 'attention', 'self', 'value', 'kernel']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_17', 'intermediate', 'dense', 'bias']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_17', 'intermediate', 'dense', 'kernel']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_17', 'output', 'LayerNorm', 'beta']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_17', 'output', 'LayerNorm', 'gamma']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_17', 'output', 'dense', 'bias']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_17', 'output', 'dense', 'kernel']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_18', 'attention', 'output', 'LayerNorm', 'beta']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_18', 'attention', 'output', 'LayerNorm', 'gamma']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_18', 'attention', 'output', 'dense', 'bias']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_18', 'attention', 'output', 'dense', 'kernel']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_18', 'attention', 'self', 'key', 'bias']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_18', 'attention', 'self', 'key', 'kernel']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_18', 'attention', 'self', 'query', 'bias']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_18', 'attention', 'self', 'query', 'kernel']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_18', 'attention', 'self', 'value', 'bias']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_18', 'attention', 'self', 'value', 'kernel']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_18', 'intermediate', 'dense', 'bias']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_18', 'intermediate', 'dense', 'kernel']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_18', 'output', 'LayerNorm', 'beta']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_18', 'output', 'LayerNorm', 'gamma']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_18', 'output', 'dense', 'bias']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_18', 'output', 'dense', 'kernel']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_19', 'attention', 'output', 'LayerNorm', 'beta']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_19', 'attention', 'output', 'LayerNorm', 'gamma']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_19', 'attention', 'output', 'dense', 'bias']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_19', 'attention', 'output', 'dense', 'kernel']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_19', 'attention', 'self', 'key', 'bias']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_19', 'attention', 'self', 'key', 'kernel']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_19', 'attention', 'self', 'query', 'bias']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_19', 'attention', 'self', 'query', 'kernel']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_19', 'attention', 'self', 'value', 'bias']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_19', 'attention', 'self', 'value', 'kernel']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_19', 'intermediate', 'dense', 'bias']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_19', 'intermediate', 'dense', 'kernel']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_19', 'output', 'LayerNorm', 'beta']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_19', 'output', 'LayerNorm', 'gamma']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_19', 'output', 'dense', 'bias']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_19', 'output', 'dense', 'kernel']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_2', 'attention', 'output', 'LayerNorm', 'beta']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_2', 'attention', 'output', 'LayerNorm', 'gamma']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_2', 'attention', 'output', 'dense', 'bias']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_2', 'attention', 'output', 'dense', 'kernel']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_2', 'attention', 'self', 'key', 'bias']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_2', 'attention', 'self', 'key', 'kernel']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_2', 'attention', 'self', 'query', 'bias']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_2', 'attention', 'self', 'query', 'kernel']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_2', 'attention', 'self', 'value', 'bias']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_2', 'attention', 'self', 'value', 'kernel']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_2', 'intermediate', 'dense', 'bias']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_2', 'intermediate', 'dense', 'kernel']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_2', 'output', 'LayerNorm', 'beta']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_2', 'output', 'LayerNorm', 'gamma']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_2', 'output', 'dense', 'bias']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_2', 'output', 'dense', 'kernel']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_20', 'attention', 'output', 'LayerNorm', 'beta']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_20', 'attention', 'output', 'LayerNorm', 'gamma']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_20', 'attention', 'output', 'dense', 'bias']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_20', 'attention', 'output', 'dense', 'kernel']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_20', 'attention', 'self', 'key', 'bias']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_20', 'attention', 'self', 'key', 'kernel']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_20', 'attention', 'self', 'query', 'bias']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_20', 'attention', 'self', 'query', 'kernel']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_20', 'attention', 'self', 'value', 'bias']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_20', 'attention', 'self', 'value', 'kernel']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_20', 'intermediate', 'dense', 'bias']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_20', 'intermediate', 'dense', 'kernel']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_20', 'output', 'LayerNorm', 'beta']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_20', 'output', 'LayerNorm', 'gamma']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_20', 'output', 'dense', 'bias']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_20', 'output', 'dense', 'kernel']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_21', 'attention', 'output', 'LayerNorm', 'beta']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_21', 'attention', 'output', 'LayerNorm', 'gamma']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_21', 'attention', 'output', 'dense', 'bias']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_21', 'attention', 'output', 'dense', 'kernel']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_21', 'attention', 'self', 'key', 'bias']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_21', 'attention', 'self', 'key', 'kernel']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_21', 'attention', 'self', 'query', 'bias']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_21', 'attention', 'self', 'query', 'kernel']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_21', 'attention', 'self', 'value', 'bias']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_21', 'attention', 'self', 'value', 'kernel']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_21', 'intermediate', 'dense', 'bias']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_21', 'intermediate', 'dense', 'kernel']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_21', 'output', 'LayerNorm', 'beta']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_21', 'output', 'LayerNorm', 'gamma']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_21', 'output', 'dense', 'bias']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_21', 'output', 'dense', 'kernel']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_22', 'attention', 'output', 'LayerNorm', 'beta']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_22', 'attention', 'output', 'LayerNorm', 'gamma']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_22', 'attention', 'output', 'dense', 'bias']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_22', 'attention', 'output', 'dense', 'kernel']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_22', 'attention', 'self', 'key', 'bias']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_22', 'attention', 'self', 'key', 'kernel']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_22', 'attention', 'self', 'query', 'bias']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_22', 'attention', 'self', 'query', 'kernel']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_22', 'attention', 'self', 'value', 'bias']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_22', 'attention', 'self', 'value', 'kernel']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_22', 'intermediate', 'dense', 'bias']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_22', 'intermediate', 'dense', 'kernel']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_22', 'output', 'LayerNorm', 'beta']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_22', 'output', 'LayerNorm', 'gamma']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_22', 'output', 'dense', 'bias']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_22', 'output', 'dense', 'kernel']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_23', 'attention', 'output', 'LayerNorm', 'beta']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_23', 'attention', 'output', 'LayerNorm', 'gamma']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_23', 'attention', 'output', 'dense', 'bias']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_23', 'attention', 'output', 'dense', 'kernel']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_23', 'attention', 'self', 'key', 'bias']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_23', 'attention', 'self', 'key', 'kernel']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_23', 'attention', 'self', 'query', 'bias']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_23', 'attention', 'self', 'query', 'kernel']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_23', 'attention', 'self', 'value', 'bias']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_23', 'attention', 'self', 'value', 'kernel']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_23', 'intermediate', 'dense', 'bias']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_23', 'intermediate', 'dense', 'kernel']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_23', 'output', 'LayerNorm', 'beta']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_23', 'output', 'LayerNorm', 'gamma']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_23', 'output', 'dense', 'bias']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_23', 'output', 'dense', 'kernel']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_3', 'attention', 'output', 'LayerNorm', 'beta']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_3', 'attention', 'output', 'LayerNorm', 'gamma']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_3', 'attention', 'output', 'dense', 'bias']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_3', 'attention', 'output', 'dense', 'kernel']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_3', 'attention', 'self', 'key', 'bias']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_3', 'attention', 'self', 'key', 'kernel']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_3', 'attention', 'self', 'query', 'bias']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_3', 'attention', 'self', 'query', 'kernel']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_3', 'attention', 'self', 'value', 'bias']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_3', 'attention', 'self', 'value', 'kernel']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_3', 'intermediate', 'dense', 'bias']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_3', 'intermediate', 'dense', 'kernel']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_3', 'output', 'LayerNorm', 'beta']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_3', 'output', 'LayerNorm', 'gamma']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_3', 'output', 'dense', 'bias']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_3', 'output', 'dense', 'kernel']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_4', 'attention', 'output', 'LayerNorm', 'beta']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_4', 'attention', 'output', 'LayerNorm', 'gamma']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_4', 'attention', 'output', 'dense', 'bias']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_4', 'attention', 'output', 'dense', 'kernel']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_4', 'attention', 'self', 'key', 'bias']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_4', 'attention', 'self', 'key', 'kernel']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_4', 'attention', 'self', 'query', 'bias']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_4', 'attention', 'self', 'query', 'kernel']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_4', 'attention', 'self', 'value', 'bias']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_4', 'attention', 'self', 'value', 'kernel']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_4', 'intermediate', 'dense', 'bias']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_4', 'intermediate', 'dense', 'kernel']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_4', 'output', 'LayerNorm', 'beta']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_4', 'output', 'LayerNorm', 'gamma']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_4', 'output', 'dense', 'bias']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_4', 'output', 'dense', 'kernel']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_5', 'attention', 'output', 'LayerNorm', 'beta']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_5', 'attention', 'output', 'LayerNorm', 'gamma']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_5', 'attention', 'output', 'dense', 'bias']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_5', 'attention', 'output', 'dense', 'kernel']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_5', 'attention', 'self', 'key', 'bias']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_5', 'attention', 'self', 'key', 'kernel']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_5', 'attention', 'self', 'query', 'bias']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_5', 'attention', 'self', 'query', 'kernel']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_5', 'attention', 'self', 'value', 'bias']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_5', 'attention', 'self', 'value', 'kernel']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_5', 'intermediate', 'dense', 'bias']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_5', 'intermediate', 'dense', 'kernel']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_5', 'output', 'LayerNorm', 'beta']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_5', 'output', 'LayerNorm', 'gamma']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_5', 'output', 'dense', 'bias']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_5', 'output', 'dense', 'kernel']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_6', 'attention', 'output', 'LayerNorm', 'beta']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_6', 'attention', 'output', 'LayerNorm', 'gamma']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_6', 'attention', 'output', 'dense', 'bias']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_6', 'attention', 'output', 'dense', 'kernel']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_6', 'attention', 'self', 'key', 'bias']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_6', 'attention', 'self', 'key', 'kernel']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_6', 'attention', 'self', 'query', 'bias']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_6', 'attention', 'self', 'query', 'kernel']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_6', 'attention', 'self', 'value', 'bias']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_6', 'attention', 'self', 'value', 'kernel']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_6', 'intermediate', 'dense', 'bias']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_6', 'intermediate', 'dense', 'kernel']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_6', 'output', 'LayerNorm', 'beta']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_6', 'output', 'LayerNorm', 'gamma']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_6', 'output', 'dense', 'bias']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_6', 'output', 'dense', 'kernel']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_7', 'attention', 'output', 'LayerNorm', 'beta']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_7', 'attention', 'output', 'LayerNorm', 'gamma']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_7', 'attention', 'output', 'dense', 'bias']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_7', 'attention', 'output', 'dense', 'kernel']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_7', 'attention', 'self', 'key', 'bias']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_7', 'attention', 'self', 'key', 'kernel']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_7', 'attention', 'self', 'query', 'bias']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_7', 'attention', 'self', 'query', 'kernel']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_7', 'attention', 'self', 'value', 'bias']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_7', 'attention', 'self', 'value', 'kernel']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_7', 'intermediate', 'dense', 'bias']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_7', 'intermediate', 'dense', 'kernel']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_7', 'output', 'LayerNorm', 'beta']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_7', 'output', 'LayerNorm', 'gamma']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_7', 'output', 'dense', 'bias']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_7', 'output', 'dense', 'kernel']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_8', 'attention', 'output', 'LayerNorm', 'beta']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_8', 'attention', 'output', 'LayerNorm', 'gamma']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_8', 'attention', 'output', 'dense', 'bias']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_8', 'attention', 'output', 'dense', 'kernel']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_8', 'attention', 'self', 'key', 'bias']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_8', 'attention', 'self', 'key', 'kernel']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_8', 'attention', 'self', 'query', 'bias']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_8', 'attention', 'self', 'query', 'kernel']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_8', 'attention', 'self', 'value', 'bias']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_8', 'attention', 'self', 'value', 'kernel']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_8', 'intermediate', 'dense', 'bias']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_8', 'intermediate', 'dense', 'kernel']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_8', 'output', 'LayerNorm', 'beta']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_8', 'output', 'LayerNorm', 'gamma']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_8', 'output', 'dense', 'bias']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_8', 'output', 'dense', 'kernel']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_9', 'attention', 'output', 'LayerNorm', 'beta']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_9', 'attention', 'output', 'LayerNorm', 'gamma']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_9', 'attention', 'output', 'dense', 'bias']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_9', 'attention', 'output', 'dense', 'kernel']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_9', 'attention', 'self', 'key', 'bias']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_9', 'attention', 'self', 'key', 'kernel']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_9', 'attention', 'self', 'query', 'bias']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_9', 'attention', 'self', 'query', 'kernel']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_9', 'attention', 'self', 'value', 'bias']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_9', 'attention', 'self', 'value', 'kernel']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_9', 'intermediate', 'dense', 'bias']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_9', 'intermediate', 'dense', 'kernel']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_9', 'output', 'LayerNorm', 'beta']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_9', 'output', 'LayerNorm', 'gamma']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_9', 'output', 'dense', 'bias']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_9', 'output', 'dense', 'kernel']\n",
            "Initialize PyTorch weight ['bert', 'pooler', 'dense', 'bias']\n",
            "Initialize PyTorch weight ['bert', 'pooler', 'dense', 'kernel']\n",
            "Initialize PyTorch weight ['cls', 'predictions', 'output_bias']\n",
            "Initialize PyTorch weight ['cls', 'predictions', 'transform', 'LayerNorm', 'beta']\n",
            "Initialize PyTorch weight ['cls', 'predictions', 'transform', 'LayerNorm', 'gamma']\n",
            "Initialize PyTorch weight ['cls', 'predictions', 'transform', 'dense', 'bias']\n",
            "Initialize PyTorch weight ['cls', 'predictions', 'transform', 'dense', 'kernel']\n",
            "Initialize PyTorch weight ['cls', 'seq_relationship', 'output_bias']\n",
            "Initialize PyTorch weight ['cls', 'seq_relationship', 'output_weights']\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Save PyTorch model to ./pytorch_model.bin\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fCmHkPcqx2YJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!rm bert_model.ckpt.data-00000-of-00001\n",
        "!rm bert_model.ckpt.index\n",
        "!rm bert_model.ckpt.meta"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rJTWI8IJs_Ae",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "os.chdir('../')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_D9ZuJlBxfCi",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 258
        },
        "outputId": "5e3f97a2-54e5-47ef-8eb4-e026da1a1cf1"
      },
      "source": [
        "!python train.py --data_dir data/task1/ --bert_model_dir model/ --model_dir experiments/base_model"
      ],
      "execution_count": 95,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "device: cuda, n_gpu: 1, 16-bits training: False\n",
            "Loading the datasets...\n",
            "loading vocabulary file model/vocab.txt\n",
            "Traceback (most recent call last):\n",
            "  File \"train.py\", line 168, in <module>\n",
            "    train_data = data_loader.load_data('train')\n",
            "  File \"/content/BERT-keyphrase-extraction/data_loader.py\", line 83, in load_data\n",
            "    self.load_sentences_tags(sentences_file, tags_path, data)\n",
            "  File \"/content/BERT-keyphrase-extraction/data_loader.py\", line 51, in load_sentences_tags\n",
            "    sentences.append(self.tokenizer.convert_tokens_to_ids(tokens))\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/pytorch_pretrained_bert/tokenization.py\", line 121, in convert_tokens_to_ids\n",
            "    ids.append(self.vocab[token])\n",
            "KeyError: 'nonzero'\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "drpWiw6f1Dyg",
        "colab_type": "text"
      },
      "source": [
        "There is a difference in the vocabulary from the SciBERT models so, we use it instead for this dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "klAa_vyfxgHv",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "042fa08a-a520-4eca-dc74-fb44ccd1e91d"
      },
      "source": [
        "os.chdir('../')\n",
        "!tar -C BERT-keyphrase-extraction/model -xvf scibert_scivocab_uncased.tar"
      ],
      "execution_count": 96,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "scibert_scivocab_uncased/\n",
            "scibert_scivocab_uncased/weights.tar.gz\n",
            "scibert_scivocab_uncased/vocab.txt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FsdRXz5-2IUe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "os.chdir('BERT-keyphrase-extraction/model')\n",
        "!rm vocab.txt\n",
        "!mv ./scibert_scivocab_uncased/vocab.txt ./"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z-ixN4lI2LmZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!rm -r scibert_scivocab_uncased\n",
        "os.chdir('../')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MU4C2WUq-DsT",
        "colab_type": "text"
      },
      "source": [
        "Now, we need to refer to use the recommended values of batch size of 4 and sequence length of 512, with 6 epochs, if GPU's VRAM is around 11 GB. (changed in experiments/base_model/params.json)\n",
        "But, even after reducing that we still get CUDA out of Memory error so we use batch size of $2(changed)$ and sequence length of 512, with 50 epochs (`min_epoch_num:25` and `patience_num:10`). Learning rate = $3e^{-5}$(unchanged)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V1Hyj6-A2h9g",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "060b2b4a-acf7-470b-d767-dd3dbf5268a3"
      },
      "source": [
        "!python train.py --data_dir data/task1/ --bert_model_dir model/ --model_dir experiments/base_model"
      ],
      "execution_count": 105,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "device: cuda, n_gpu: 1, 16-bits training: False\n",
            "Loading the datasets...\n",
            "loading vocabulary file model/vocab.txt\n",
            "loading archive file model/\n",
            "Model config {\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 1024,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 4096,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"num_attention_heads\": 16,\n",
            "  \"num_hidden_layers\": 24,\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n",
            "Weights of BertForTokenClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']\n",
            "Weights from pretrained model not used in BertForTokenClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
            "Starting training for 50 epoch(s)\n",
            "Epoch 1/50\n",
            "/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
            "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n",
            "100% 199/199 [00:50<00:00,  3.95it/s, loss=0.683]\n",
            "- Train metrics: loss: 00.69; f1: 00.00\n",
            "- Val metrics: loss: 00.83; f1: 00.00\n",
            "Epoch 2/50\n",
            "100% 199/199 [00:49<00:00,  4.06it/s, loss=0.673]\n",
            "- Train metrics: loss: 00.69; f1: 00.00\n",
            "- Val metrics: loss: 00.82; f1: 00.00\n",
            "Epoch 3/50\n",
            "100% 199/199 [00:48<00:00,  4.07it/s, loss=0.670]\n",
            "- Train metrics: loss: 00.69; f1: 00.00\n",
            "- Val metrics: loss: 00.81; f1: 00.00\n",
            "Epoch 4/50\n",
            "100% 199/199 [00:49<00:00,  4.05it/s, loss=0.673]\n",
            "- Train metrics: loss: 00.68; f1: 00.00\n",
            "- Val metrics: loss: 00.69; f1: 00.00\n",
            "Epoch 5/50\n",
            "100% 199/199 [00:50<00:00,  3.92it/s, loss=0.675]\n",
            "- Train metrics: loss: 00.68; f1: 00.00\n",
            "- Val metrics: loss: 00.79; f1: 00.00\n",
            "Epoch 6/50\n",
            "100% 199/199 [00:49<00:00,  4.02it/s, loss=0.671]\n",
            "- Train metrics: loss: 00.69; f1: 00.00\n",
            "- Val metrics: loss: 00.81; f1: 00.00\n",
            "Epoch 7/50\n",
            "100% 199/199 [00:50<00:00,  3.90it/s, loss=0.668]\n",
            "- Train metrics: loss: 00.69; f1: 00.00\n",
            "- Val metrics: loss: 00.82; f1: 00.00\n",
            "Epoch 8/50\n",
            "100% 199/199 [00:49<00:00,  3.98it/s, loss=0.667]\n",
            "- Train metrics: loss: 00.69; f1: 00.00\n",
            "- Val metrics: loss: 00.82; f1: 00.00\n",
            "Epoch 9/50\n",
            "100% 199/199 [00:50<00:00,  3.92it/s, loss=0.666]\n",
            "- Train metrics: loss: 00.68; f1: 00.00\n",
            "- Val metrics: loss: 00.81; f1: 00.00\n",
            "Epoch 10/50\n",
            "100% 199/199 [00:50<00:00,  3.98it/s, loss=0.665]\n",
            "- Train metrics: loss: 00.68; f1: 00.00\n",
            "- Val metrics: loss: 00.80; f1: 00.00\n",
            "Epoch 11/50\n",
            "100% 199/199 [00:50<00:00,  3.97it/s, loss=0.665]\n",
            "- Train metrics: loss: 00.68; f1: 00.00\n",
            "- Val metrics: loss: 00.80; f1: 00.00\n",
            "Epoch 12/50\n",
            "100% 199/199 [00:49<00:00,  4.00it/s, loss=0.664]\n",
            "- Train metrics: loss: 00.68; f1: 00.00\n",
            "- Val metrics: loss: 00.79; f1: 00.00\n",
            "Epoch 13/50\n",
            "100% 199/199 [00:49<00:00,  4.03it/s, loss=0.664]\n",
            "- Train metrics: loss: 00.67; f1: 00.00\n",
            "- Val metrics: loss: 00.79; f1: 00.00\n",
            "Epoch 14/50\n",
            "100% 199/199 [00:49<00:00,  4.06it/s, loss=0.663]\n",
            "- Train metrics: loss: 00.67; f1: 00.00\n",
            "- Val metrics: loss: 00.78; f1: 00.00\n",
            "Epoch 15/50\n",
            "100% 199/199 [00:49<00:00,  4.01it/s, loss=0.663]\n",
            "- Train metrics: loss: 00.67; f1: 00.00\n",
            "- Val metrics: loss: 00.78; f1: 00.00\n",
            "Epoch 16/50\n",
            "100% 199/199 [00:49<00:00,  4.06it/s, loss=0.663]\n",
            "- Train metrics: loss: 00.67; f1: 00.00\n",
            "- Val metrics: loss: 00.77; f1: 00.00\n",
            "Epoch 17/50\n",
            "100% 199/199 [00:48<00:00,  4.07it/s, loss=0.663]\n",
            "- Train metrics: loss: 00.67; f1: 00.00\n",
            "- Val metrics: loss: 00.77; f1: 00.00\n",
            "Epoch 18/50\n",
            "100% 199/199 [00:49<00:00,  4.01it/s, loss=0.662]\n",
            "- Train metrics: loss: 00.67; f1: 00.00\n",
            "- Val metrics: loss: 00.77; f1: 00.00\n",
            "Epoch 19/50\n",
            "100% 199/199 [00:49<00:00,  4.05it/s, loss=0.662]\n",
            "- Train metrics: loss: 00.67; f1: 00.00\n",
            "- Val metrics: loss: 00.77; f1: 00.00\n",
            "Epoch 20/50\n",
            "100% 199/199 [00:48<00:00,  4.07it/s, loss=0.662]\n",
            "- Train metrics: loss: 00.67; f1: 00.00\n",
            "- Val metrics: loss: 00.76; f1: 00.00\n",
            "Epoch 21/50\n",
            "100% 199/199 [00:49<00:00,  4.03it/s, loss=0.662]\n",
            "- Train metrics: loss: 00.66; f1: 00.00\n",
            "- Val metrics: loss: 00.76; f1: 00.00\n",
            "Epoch 22/50\n",
            "100% 199/199 [00:49<00:00,  4.05it/s, loss=0.662]\n",
            "- Train metrics: loss: 00.66; f1: 00.00\n",
            "- Val metrics: loss: 00.76; f1: 00.00\n",
            "Epoch 23/50\n",
            "100% 199/199 [00:50<00:00,  3.91it/s, loss=0.662]\n",
            "- Train metrics: loss: 00.66; f1: 00.00\n",
            "- Val metrics: loss: 00.76; f1: 00.00\n",
            "Epoch 24/50\n",
            "100% 199/199 [00:49<00:00,  4.05it/s, loss=0.661]\n",
            "- Train metrics: loss: 00.66; f1: 00.00\n",
            "- Val metrics: loss: 00.76; f1: 00.00\n",
            "Epoch 25/50\n",
            "100% 199/199 [00:49<00:00,  4.04it/s, loss=0.666]\n",
            "- Train metrics: loss: 00.69; f1: 00.00\n",
            "- Val metrics: loss: 00.83; f1: 00.00\n",
            "Epoch 26/50\n",
            "100% 199/199 [00:50<00:00,  3.97it/s, loss=0.664]\n",
            "- Train metrics: loss: 00.68; f1: 00.00\n",
            "- Val metrics: loss: 00.80; f1: 00.00\n",
            "Best val f1: 00.00\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bP7NpLz99nWz",
        "colab_type": "text"
      },
      "source": [
        "The above training and evaluations which is definitely flawed leads us to 22 interpretations which have been mentioned below.\n",
        "\n",
        "* This flawed output ($f1 score = 0$) depicts that a `gridSearch` is needed for better Hyperparameter tuning and the bindings between SciBERT (produced by AllenAI) and BERT (google) need to be adjusted in order for the models to work on different codes. \n",
        "* Both models have different vocab list which leads to differences in both the implementations.\n",
        "* Another solution is to use FastAI functions (such as `lr_find()` and `fit_one_cycle()`) to get better hyperparameters instead of gridSearchCV. This way is faster than generic GridSearchCV. These functions have been also used in computer vision task as can be seen here for [Wide ResNet22](https://github.com/AnshMittal1811/Pytorch/blob/master/Udemy%20-%20Pytorch%20zero%20to%20gans/L6%20ResNets%20and%20WRN22.ipynb)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b3o3guDq8w6c",
        "colab_type": "text"
      },
      "source": [
        "-----"
      ]
    }
  ]
}